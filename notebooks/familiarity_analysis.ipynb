{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and defenitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re  # for filename sanitization\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from globals import survey_data, principle_columns, regulation_columns, principles, regulation_names, familiarity_levels, demographics\n",
    "\n",
    "# Define file path\n",
    "file_path = survey_data\n",
    "\n",
    "colors = ['#FFFF99', '#FFEDA0', '#C7E9B4', '#7FCDBB', '#2C7FB8']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A and B.2 Familiarity with Ethics Principles \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Successfully read {file_path}.\")\n",
    "        return df\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read {file_path} with utf-8 encoding.\")\n",
    "        return None\n",
    "\n",
    "# ---------------- Chart Creation Functions ---------------- #\n",
    "\n",
    "def create_chart(data, category, demographic, track):\n",
    "    \"\"\"Creates and saves an individual horizontal stacked bar chart.\"\"\"\n",
    "    if data.empty:\n",
    "        print(f\"No data available for {category} in {demographic} - Track {track}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    y_pos = np.arange(len(data.index)) * 1.2\n",
    "    bar_height = 0.8\n",
    "    cumulative = np.zeros(len(data.index))\n",
    "\n",
    "    for i, level in enumerate(familiarity_levels):\n",
    "        values = data[level].values\n",
    "        ax.barh(y_pos, values, left=cumulative, height=bar_height, label=level, color=colors[i])\n",
    "        cumulative += values\n",
    "\n",
    "    ax.set_xlabel('', fontweight='bold', fontname='Times New Roman')\n",
    "    ax.set_title(f'Familiarity with AI Ethics Principles - {demographic}: {category} - Track {track}',\n",
    "                 fontweight='bold', fontsize=16, fontname='Times New Roman')\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    for i, container in enumerate(ax.containers):\n",
    "        text_color = 'black' if i < 3 else 'white'\n",
    "        ax.bar_label(container, label_type='center', fmt='%.1f%%', fontname='Times New Roman',\n",
    "                     fontweight='bold', fontsize=10, padding=2, color=text_color)\n",
    "\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(data.index, fontweight='bold', fontname='Times New Roman')\n",
    "    ax.set_ylim(y_pos.min() - bar_height/2, y_pos.max() + bar_height/2)\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=level) for color, level in zip(colors, familiarity_levels)]\n",
    "    fig.legend(handles=legend_patches, loc='lower center', bbox_to_anchor=(0.5, -0.12), \n",
    "               ncol=5, fontsize=8, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    \n",
    "    # Precompute safe strings to avoid f-string backslash issues\n",
    "    safe_demographic = re.sub(r'[<>:\"/\\\\|?*]', '_', demographic)\n",
    "    safe_category = re.sub(r'[<>:\"/\\\\|?*]', '_', category)\n",
    "    plt.savefig(\n",
    "        f'figures/familiarity_analysis/principle_familiarity/{track}.2.1/{track}.2.1_{safe_demographic}_{safe_category}_Track_{track}.png',\n",
    "        dpi=300, bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def create_combined_chart(all_data, demographic, track, graphs_per_row=3):\n",
    "    \"\"\"\n",
    "    Creates and saves a combined chart that arranges multiple subplots in a grid.\n",
    "    The number of graphs per row can be controlled with the graphs_per_row parameter.\n",
    "    \"\"\"\n",
    "    num_categories = len(all_data)\n",
    "    if num_categories == 0:\n",
    "        print(f\"No categories to plot for {demographic} - Track {track}\")\n",
    "        return\n",
    "\n",
    "    # Determine number of rows needed\n",
    "    num_rows = (num_categories + graphs_per_row - 1) // graphs_per_row\n",
    "    total_axes = num_rows * graphs_per_row\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, graphs_per_row, \n",
    "                             figsize=(10 * graphs_per_row, 4 * num_rows), \n",
    "                             sharey=False)\n",
    "    # Ensure axes is always a 2D array\n",
    "    if num_rows == 1:\n",
    "        axes = np.array([axes])\n",
    "    if graphs_per_row == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "\n",
    "    fig.suptitle(f'Familiarity with AI Ethics Principles by {demographic} - Track {track}',\n",
    "                 fontweight='bold', fontsize=16, fontname='Times New Roman')\n",
    "\n",
    "    category_items = list(all_data.items())\n",
    "    for idx, (category, data) in enumerate(category_items):\n",
    "        row = idx // graphs_per_row\n",
    "        col = idx % graphs_per_row\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        y_pos = np.arange(len(data.index)) * 1.2\n",
    "        bar_height = 0.8\n",
    "        cumulative = np.zeros(len(data.index))\n",
    "\n",
    "        for i, level in enumerate(familiarity_levels):\n",
    "            values = data[level].values\n",
    "            ax.barh(y_pos, values, left=cumulative, height=bar_height, label=level, color=colors[i])\n",
    "            cumulative += values\n",
    "\n",
    "        ax.set_xlabel('', fontweight='bold', fontname='Times New Roman', fontsize=10)\n",
    "        ax.set_title(category, fontweight='bold', fontsize=12, fontname='Times New Roman')\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        for i, container in enumerate(ax.containers):\n",
    "            text_color = 'black' if i < 3 else 'white'\n",
    "            ax.bar_label(container, label_type='center', fmt='%.1f%%', fontname='Times New Roman',\n",
    "                         fontweight='bold', fontsize=8, padding=2, color=text_color)\n",
    "\n",
    "        # Only show y-axis tick labels on the first column to reduce clutter\n",
    "        if col == 0:\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(data.index, fontweight='bold', fontname='Times New Roman', fontsize=8)\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "        ax.set_ylim(y_pos.min() - bar_height/2, y_pos.max() + bar_height/2)\n",
    "        for text in ax.get_xticklabels():\n",
    "            text.set_fontname('Times New Roman')\n",
    "            text.set_fontsize(8)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for idx in range(num_categories, total_axes):\n",
    "        row = idx // graphs_per_row\n",
    "        col = idx % graphs_per_row\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=level) for color, level in zip(colors, familiarity_levels)]\n",
    "    fig.legend(handles=legend_patches, loc='lower center', bbox_to_anchor=(0.5, -0.05),\n",
    "               ncol=5, fontsize=8, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15, wspace=0.3)\n",
    "    \n",
    "    safe_demographic = re.sub(r'[<>:\"/\\\\|?*]', '_', demographic)\n",
    "    plt.savefig(\n",
    "        f'figures/familiarity_analysis/principle_familiarity/{track}.2.1/{track}.2.1_{safe_demographic}_combined_Track_{track}.png',\n",
    "        dpi=300, bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ---------------- Data Processing and Statistics ---------------- #\n",
    "\n",
    "def process_and_create_charts(df, demographic, track):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame to group responses by a demographic category,\n",
    "    generates individual charts for each group, and returns a dictionary of the data.\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "\n",
    "    if demographic == 'Gender':\n",
    "        categories = df.iloc[:, demographics[demographic]['column']].dropna().unique()\n",
    "        df['GroupedCategory'] = df.iloc[:, demographics[demographic]['column']]\n",
    "    else:\n",
    "        categories = set(demographics[demographic]['mapping'].values())\n",
    "        df['GroupedCategory'] = df.iloc[:, demographics[demographic]['column']].map(demographics[demographic]['mapping'])\n",
    "\n",
    "    for category in categories:\n",
    "        df_category = df[df['GroupedCategory'] == category]\n",
    "        data = pd.DataFrame(index=principles, columns=familiarity_levels)\n",
    "        valid_columns = principle_columns[track]\n",
    "        df_track = df_category.dropna(subset=[df_category.columns[col] for col in valid_columns])\n",
    "        total_respondents = len(df_track)\n",
    "\n",
    "        for idx, principle in enumerate(principles):\n",
    "            if idx < len(valid_columns):\n",
    "                column = valid_columns[idx]\n",
    "                if column < df_track.shape[1]:\n",
    "                    counts = df_track.iloc[:, column].value_counts()\n",
    "                    for level, count in counts.items():\n",
    "                        if level in familiarity_levels:\n",
    "                            data.loc[principle, level] = (count / total_respondents) * 100\n",
    "        data = data.fillna(0)\n",
    "        data = data[familiarity_levels]\n",
    "        all_data[category] = data\n",
    "\n",
    "        create_chart(data, category, demographic, track)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def print_stats(all_data, demographic, track):\n",
    "    \"\"\"\n",
    "    Prints summary statistics for the processed data.\n",
    "    \"\"\"\n",
    "    stats_summary = f\"Statistics for {demographic} - Track {track}:\\n\\n\"\n",
    "    for category, data in all_data.items():\n",
    "        stats_summary += f\"{category}:\\n\"\n",
    "        total_respondents = data.iloc[0].sum() / 100  \n",
    "        for principle in principles:\n",
    "            stats_summary += f\"{principle}:\\n\"\n",
    "            for level in familiarity_levels:\n",
    "                percentage = data.loc[principle, level]\n",
    "                count = round(percentage * total_respondents / 100)\n",
    "                stats_summary += f\" {level}: {count:.0f} ({percentage:.1f}%),\"\n",
    "            stats_summary += \"\\n\"\n",
    "        stats_summary += f\"Total respondents: {total_respondents:.0f}\\n\"\n",
    "        stats_summary += \"\\n\" + \"=\"*50 + \"\\n\"\n",
    "    \n",
    "    print(stats_summary)\n",
    "\n",
    "\n",
    "def process_combined_tracks(df, track):\n",
    "    \"\"\"\n",
    "    Processes data for all participants in a single track (A or B) and creates a combined chart.\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    data = pd.DataFrame(index=principles, columns=familiarity_levels)\n",
    "    valid_columns = principle_columns[track]\n",
    "    df_track = df.dropna(subset=[df.columns[col] for col in valid_columns])\n",
    "    total_respondents = len(df_track)\n",
    "\n",
    "    for idx, principle in enumerate(principles):\n",
    "        if idx < len(valid_columns):\n",
    "            column = valid_columns[idx]\n",
    "            if column < df_track.shape[1]:\n",
    "                counts = df_track.iloc[:, column].value_counts()\n",
    "                for level, count in counts.items():\n",
    "                    if level in familiarity_levels:\n",
    "                        data.loc[principle, level] = (count / total_respondents) * 100\n",
    "                    else:\n",
    "                        print(f\"Unexpected familiarity level '{level}' for {principle}\")\n",
    "    data = data.fillna(0)\n",
    "    data = data[familiarity_levels]\n",
    "    all_data['All Participants'] = data\n",
    "\n",
    "    create_combined_chart(all_data, 'All Participants', track, graphs_per_row=3)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def process_combined_all_tracks(df):\n",
    "    \"\"\"\n",
    "    Processes data across both Track A and Track B and creates a combined chart.\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    combined_data = pd.DataFrame(index=principles, columns=familiarity_levels)\n",
    "    total_respondents = 0\n",
    "\n",
    "    # Accumulate total respondents across both tracks\n",
    "    for track in ['A', 'B']:\n",
    "        valid_columns = principle_columns[track]\n",
    "        df_track = df.dropna(subset=[df.columns[col] for col in valid_columns])\n",
    "        track_respondents = len(df_track)\n",
    "        total_respondents += track_respondents\n",
    "\n",
    "        for idx, principle in enumerate(principles):\n",
    "            if idx < len(valid_columns):\n",
    "                column = valid_columns[idx]\n",
    "                if column < df_track.shape[1]:\n",
    "                    counts = df_track.iloc[:, column].value_counts()\n",
    "                    for level, count in counts.items():\n",
    "                        if level in familiarity_levels:\n",
    "                            current = combined_data.loc[principle, level]\n",
    "                            if pd.isna(current):\n",
    "                                current = 0\n",
    "                            combined_data.loc[principle, level] = current + (count / total_respondents) * 100\n",
    "                        else:\n",
    "                            print(f\"Unexpected familiarity level '{level}' for {principle}\")\n",
    "    combined_data = combined_data.fillna(0)\n",
    "    combined_data = combined_data[familiarity_levels]\n",
    "    all_data['All Participants'] = combined_data\n",
    "\n",
    "    create_combined_chart(all_data, 'All Participants', 'Combined', graphs_per_row=3)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# ---------------- Main Execution ---------------- #\n",
    "\n",
    "df = read_and_clean_csv(file_path)\n",
    "if df is not None:\n",
    "    for track in ['A', 'B']:\n",
    "        print(f\"Processing Track {track}...\")\n",
    "        track_columns = principle_columns[track]\n",
    "        df_track = df.dropna(subset=[df.columns[col] for col in track_columns])\n",
    "        for demographic in demographics:\n",
    "            print(f\"Processing {demographic}...\")\n",
    "            all_data = process_and_create_charts(df_track, demographic, track)\n",
    "            create_combined_chart(all_data, demographic, track, graphs_per_row=3)\n",
    "            print_stats(all_data, demographic, track)\n",
    "        combined_track_data = process_combined_tracks(df, track)\n",
    "    \n",
    "    combined_all_tracks_data = process_combined_all_tracks(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C Familiarity with Ethics Initiatives \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Successfully read {file_path}.\")\n",
    "        return df\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read {file_path} with utf-8 encoding.\")\n",
    "        return None\n",
    "\n",
    "def create_chart(data, category, demographic):\n",
    "    if data.empty:\n",
    "        print(f\"No data available for {category} in {demographic}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(18, 6))  # Increase figure size for larger output\n",
    "\n",
    "    y_pos = np.arange(len(data.index)) * 1.2  # Increase spacing between bars\n",
    "    bar_height = 0.8  # Bar height\n",
    "\n",
    "    cumulative = np.zeros(len(data.index))\n",
    "\n",
    "    for i, level in enumerate(familiarity_levels):\n",
    "        values = data[level].values\n",
    "        ax.barh(y_pos, values, left=cumulative, height=bar_height, label=level, color=colors[i])\n",
    "        cumulative += values\n",
    "\n",
    "    ax.set_xlabel('', fontweight='bold', fontname='Times New Roman')\n",
    "    ax.set_title(f'Familiarity with AI Governance Initiatives - {demographic}: {category}',\n",
    "                 fontweight='bold', fontsize=22, fontname='Times New Roman', pad=20)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    for i, container in enumerate(ax.containers):\n",
    "        text_color = 'black' if i < 3 else 'white'\n",
    "        ax.bar_label(container, label_type='center', fmt='%.1f%%', fontname='Times New Roman',\n",
    "                     fontweight='bold', fontsize=12, padding=4, color=text_color)\n",
    "\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(data.index, fontweight='bold', fontname='Times New Roman', fontsize=14)\n",
    "    ax.set_ylim(y_pos.min() - bar_height/2, y_pos.max() + bar_height/2)\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=level) for color, level in zip(colors, familiarity_levels)]\n",
    "    fig.legend(handles=legend_patches, loc='lower center', bbox_to_anchor=(0.5, -0.15),\n",
    "               ncol=5, fontsize=12, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "\n",
    "    # Save the chart as an image file\n",
    "    safe_category = re.sub(r'[<>:\"/\\\\|?*]', '_', category)\n",
    "    safe_demographic = re.sub(r'[<>:\"/\\\\|?*]', '_', demographic)\n",
    "    plt.savefig(f'figures/familiarity_analysis/governance_familiarity/C.2_{safe_demographic}_{safe_category}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_combined_chart(all_data, demographic, graphs_per_row=3, categories_to_exclude=None):\n",
    "    if categories_to_exclude is None:\n",
    "        categories_to_exclude = []\n",
    "\n",
    "    filtered_data = {k: v for k, v in all_data.items() if k not in categories_to_exclude}\n",
    "    num_categories = len(filtered_data)\n",
    "\n",
    "    if num_categories == 0:\n",
    "        print(f\"No categories to plot for {demographic} after excluding {categories_to_exclude}\")\n",
    "        return\n",
    "\n",
    "    num_rows = (num_categories + graphs_per_row - 1) // graphs_per_row\n",
    "    num_categories_adjusted = num_rows * graphs_per_row if num_categories % graphs_per_row != 0 else num_categories\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, graphs_per_row, figsize=(8 * graphs_per_row, 6 * num_rows), sharey=False)\n",
    "    fig.suptitle(f'Familiarity with AI Governance Initiatives - {demographic}', \n",
    "                 fontweight='bold', fontsize=22, fontname='Times New Roman', y=0.98 if num_rows > 1 else 1.05)\n",
    "\n",
    "    category_items = list(filtered_data.items())\n",
    "    for idx, (category, data) in enumerate(category_items):\n",
    "        if idx >= num_categories_adjusted:\n",
    "            break\n",
    "\n",
    "        row = idx // graphs_per_row\n",
    "        col = idx % graphs_per_row\n",
    "        ax = axes[row, col] if num_rows > 1 else (axes[col] if graphs_per_row > 1 else axes)\n",
    "\n",
    "        y_pos = np.arange(len(data.index)) * 1.2\n",
    "        bar_height = 0.8\n",
    "        cumulative = np.zeros(len(data.index))\n",
    "\n",
    "        for i, level in enumerate(familiarity_levels):\n",
    "            values = data[level].values\n",
    "            ax.barh(y_pos, values, left=cumulative, height=bar_height, label=level, color=colors[i])\n",
    "            cumulative += values\n",
    "\n",
    "        ax.set_xlabel('', fontweight='bold', fontname='Times New Roman')\n",
    "        ax.set_title(category, fontweight='bold', fontsize=18, fontname='Times New Roman', pad=20)\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        if col == 0:\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(data.index, fontweight='bold', fontname='Times New Roman', fontsize=14)\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "        for i, container in enumerate(ax.containers):\n",
    "            text_color = 'black' if i < 3 else 'white'\n",
    "            ax.bar_label(container, label_type='center', fmt='%.1f%%', fontname='Times New Roman',\n",
    "                         fontweight='bold', fontsize=12, padding=4, color=text_color)\n",
    "\n",
    "        ax.set_ylim(y_pos.min() - bar_height/2, y_pos.max() + bar_height/2)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for idx in range(num_categories_adjusted, num_rows * graphs_per_row):\n",
    "        row = idx // graphs_per_row\n",
    "        col = idx % graphs_per_row\n",
    "        if num_rows > 1:\n",
    "            fig.delaxes(axes[row, col])\n",
    "        elif graphs_per_row > 1:\n",
    "            fig.delaxes(axes[col])\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=level) for color, level in zip(colors, familiarity_levels)]\n",
    "    fig.legend(handles=legend_patches, loc='lower center', bbox_to_anchor=(0.5, -0.01),\n",
    "               ncol=min(graphs_per_row, 5), fontsize=12, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15 if num_rows > 1 else 0.25, hspace=0.6 if num_rows > 1 else 0.25, wspace=0.2)\n",
    "\n",
    "    safe_demographic = re.sub(r'[<>:\"/\\\\|?*]', '_', demographic)\n",
    "    plt.savefig(f'figures/familiarity_analysis/governance_familiarity/C.2_{safe_demographic}_combined.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def process_and_create_charts(df, demographic):\n",
    "    all_data = {}\n",
    "\n",
    "    if demographics[demographic]['mapping']:\n",
    "        categories = set(demographics[demographic]['mapping'].values())\n",
    "        df['GroupedCategory'] = df.iloc[:, demographics[demographic]['column']].map(demographics[demographic]['mapping'])\n",
    "    else:\n",
    "        categories = df.iloc[:, demographics[demographic]['column']].dropna().unique()\n",
    "        df['GroupedCategory'] = df.iloc[:, demographics[demographic]['column']]\n",
    "\n",
    "    for category in categories:\n",
    "        df_category = df[df['GroupedCategory'] == category]\n",
    "        data = pd.DataFrame(index=regulation_names, columns=familiarity_levels)\n",
    "\n",
    "        for idx, initiative in enumerate(regulation_names):\n",
    "            if idx < len(regulation_columns):\n",
    "                column = regulation_columns[idx]\n",
    "                if column < df_category.shape[1]:\n",
    "                    valid_responses = df_category.iloc[:, column].dropna()\n",
    "                    total_respondents = len(valid_responses)\n",
    "                    if total_respondents > 0:\n",
    "                        counts = valid_responses.value_counts()\n",
    "                        for level, count in counts.items():\n",
    "                            if level in familiarity_levels:\n",
    "                                data.loc[initiative, level] = (count / total_respondents) * 100\n",
    "                    else:\n",
    "                        print(f\"Column index {column} is out of range for {initiative} in {category}\")\n",
    "\n",
    "        data = data.fillna(0)\n",
    "        data = data[familiarity_levels]\n",
    "        all_data[category] = data\n",
    "        create_chart(data, category, demographic)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def print_stats(all_data, demographic):\n",
    "    stats_summary = f\"Statistics for {demographic}:\\n\\n\"\n",
    "    for category, data in all_data.items():\n",
    "        stats_summary += f\"{category}:\\n\"\n",
    "        total_respondents = data.iloc[0].sum() / 100\n",
    "        for initiative in regulation_names:\n",
    "            stats_summary += f\"{initiative}:\\n\"\n",
    "            for level in familiarity_levels:\n",
    "                percentage = data.loc[initiative, level]\n",
    "                count = round(percentage * total_respondents / 100)\n",
    "                stats_summary += f\" {level}: {count:.0f} ({percentage:.1f}%),\"\n",
    "            stats_summary += \"\\n\"\n",
    "        stats_summary += f\"Total respondents: {total_respondents:.0f}\\n\" + \"=\"*50 + \"\\n\"\n",
    "\n",
    "    # Instead of writing to a text file, we simply print the summary\n",
    "    print(stats_summary)\n",
    "\n",
    "def process_combined_data(df):\n",
    "    all_data = {}\n",
    "    data = pd.DataFrame(index=regulation_names, columns=familiarity_levels)\n",
    "\n",
    "    for idx, initiative in enumerate(regulation_names):\n",
    "        if idx < len(regulation_columns):\n",
    "            column = regulation_columns[idx]\n",
    "            if column < df.shape[1]:\n",
    "                valid_responses = df.iloc[:, column].dropna()\n",
    "                total_respondents = len(valid_responses)\n",
    "                if total_respondents > 0:\n",
    "                    counts = valid_responses.value_counts()\n",
    "                    for level, count in counts.items():\n",
    "                        if level in familiarity_levels:\n",
    "                            data.loc[initiative, level] = (count / total_respondents) * 100\n",
    "\n",
    "    data = data.fillna(0)\n",
    "    data = data[familiarity_levels]\n",
    "    all_data['All Participants'] = data\n",
    "    create_chart(data, 'All Participants', 'Combined')\n",
    "    return all_data\n",
    "\n",
    "# Main execution\n",
    "df = read_and_clean_csv(file_path)\n",
    "if df is not None:\n",
    "    print(\"Processing all participants...\")\n",
    "    combined_data = process_combined_data(df)\n",
    "    print_stats(combined_data, 'All Participants')\n",
    "\n",
    "    for demographic in demographics:\n",
    "        print(f\"Processing {demographic}...\")\n",
    "        all_data = process_and_create_charts(df, demographic)\n",
    "        # Generate combined charts across different categories within a demographic, excluding 'Other' for 'Role'\n",
    "        categories_to_exclude = ['Other'] if demographic == 'Role' else []\n",
    "        create_combined_chart(all_data, demographic, graphs_per_row=3, categories_to_exclude=categories_to_exclude)\n",
    "        print_stats(all_data, demographic)\n",
    "\n",
    "    print(\"All graphs have been saved in the designated folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Rank Demographics Familiarity with Principles and Regultions (Requires above code to be run first), input demographic at bottom of cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################\n",
    "# Rank demographics familiarities with principles\n",
    "###################\n",
    "\n",
    "def read_and_clean_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Successfully read {file_path}.\")\n",
    "        return df\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read {file_path} with utf-8 encoding.\")\n",
    "        return None\n",
    "\n",
    "def rank_demographics_by_principle(df, track, demographic_key):\n",
    "    ranked_data = {}\n",
    "\n",
    "    # Get the demographic details from the selected key\n",
    "    demographic = demographics[demographic_key]\n",
    "    print(f\"\\nRanking {demographic_key} demographics for Track {track}...\\n\")\n",
    "\n",
    "    # Process each principle and demographic to calculate the percentage for \"Extremely Familiar\" and \"Moderately Familiar\"\n",
    "    for principle_idx, principle in enumerate(principles):\n",
    "        ranked_data[principle] = []\n",
    "\n",
    "        demographic_data = {category: 0 for category in set(demographic['mapping'].values())}  # Initialize with 0 for all categories\n",
    "\n",
    "        df['GroupedCategory'] = df.iloc[:, demographic['column']].map(demographic['mapping'])\n",
    "\n",
    "        # Iterate over each category within the demographic (e.g., 1-5 Employees, 6-20 Employees, etc.)\n",
    "        for category in set(demographic['mapping'].values()):\n",
    "            df_category = df[df['GroupedCategory'] == category]\n",
    "            valid_columns = principle_columns[track]\n",
    "            df_track = df_category.dropna(subset=[df_category.columns[valid_columns[principle_idx]]])\n",
    "\n",
    "            if not df_track.empty:\n",
    "                # Count \"Extremely Familiar\" and \"Moderately Familiar\" responses\n",
    "                counts = df_track.iloc[:, valid_columns[principle_idx]].value_counts()\n",
    "                extremely_familiar = counts.get('Extremely Familiar', 0)\n",
    "                moderately_familiar = counts.get('Moderately Familiar', 0)\n",
    "                somewhat_familiar = counts.get('Somewhat Familiar', 0)\n",
    "                total_respondents = len(df_track)\n",
    "\n",
    "                total_familiar = ((extremely_familiar + moderately_familiar + somewhat_familiar) / total_respondents) * 100\n",
    "                demographic_data[category] = total_familiar\n",
    "\n",
    "        # Rank the categories for this demographic and principle\n",
    "        ranked_categories = sorted(demographic_data.items(), key=lambda x: x[1], reverse=True)\n",
    "        ranked_data[principle].append((demographic_key, ranked_categories))\n",
    "\n",
    "    return ranked_data\n",
    "\n",
    "def display_ranked_data(ranked_data):\n",
    "    for principle, demographic_rankings in ranked_data.items():\n",
    "        print(f\"\\n### {principle} Rankings ###\\n\")\n",
    "        for demographic, rankings in demographic_rankings:\n",
    "            print(f\"{demographic}:\")\n",
    "            for category, percentage in rankings:\n",
    "                if percentage > 0:  # Only display categories with responses\n",
    "                    print(f\"  {category}: {percentage:.1f}%\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Main execution\n",
    "df = read_and_clean_csv(file_path)\n",
    "if df is not None:\n",
    "    # Select the demographic to rank (e.g., 'Company Size')\n",
    "    selected_demographic = 'Location' # This can be changed to other demographic keys like 'Location', etc.\n",
    "\n",
    "    for track in ['B']:\n",
    "        print(f\"Processing Track {track} for {selected_demographic}...\")\n",
    "        ranked_data = rank_demographics_by_principle(df, track, selected_demographic)\n",
    "        display_ranked_data(ranked_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Rank demographics familiarities with regulations\n",
    "###################\n",
    "\n",
    "def rank_by_familiarity_and_average(df, demographic):\n",
    "    # Ensure the demographic is valid\n",
    "    if demographic not in demographics:\n",
    "        print(f\"Invalid demographic: {demographic}\")\n",
    "        return\n",
    "    \n",
    "    # Prepare to store the ranking data and average scores\n",
    "    ranking_data = {}\n",
    "    avg_familiarity_by_category = {}\n",
    "\n",
    "    # Group by the selected demographic category\n",
    "    df['GroupedCategory'] = df.iloc[:, demographics[demographic]['column']].map(demographics[demographic]['mapping'])\n",
    "\n",
    "    for initiative in regulation_names:\n",
    "        initiative_scores = {}\n",
    "\n",
    "        # For each category within the demographic (e.g., US, Europe for Location)\n",
    "        for category in set(demographics[demographic]['mapping'].values()):\n",
    "            df_category = df[df['GroupedCategory'] == category]\n",
    "            if df_category.empty:\n",
    "                continue\n",
    "            \n",
    "            # Calculate the percentage of respondents for \"Extremely Familiar\", \"Moderately Familiar\", and \"Somewhat Familiar\"\n",
    "            total_respondents = len(df_category)\n",
    "            if total_respondents == 0:\n",
    "                continue\n",
    "            \n",
    "            familiarity_count = df_category.iloc[:, regulation_names.index(initiative) + regulation_columns[0]].value_counts()\n",
    "            at_least_somewhat_familiar = familiarity_count.get('Extremely Familiar', 0) + familiarity_count.get('Moderately Familiar', 0) + familiarity_count.get('Somewhat Familiar', 0)\n",
    "            \n",
    "            percentage = (at_least_somewhat_familiar / total_respondents) * 100\n",
    "            initiative_scores[category] = percentage\n",
    "\n",
    "            # Add score to the average familiarity for each category\n",
    "            if category not in avg_familiarity_by_category:\n",
    "                avg_familiarity_by_category[category] = []\n",
    "            avg_familiarity_by_category[category].append(percentage)\n",
    "        \n",
    "        # Sort the categories for this regulation based on the percentage\n",
    "        sorted_scores = sorted(initiative_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        ranking_data[initiative] = sorted_scores\n",
    "\n",
    "    # Print out the ranking results for each regulation\n",
    "    for initiative, rankings in ranking_data.items():\n",
    "        print(f\"\\nRanking for {initiative} based on '{demographic}':\")\n",
    "        for idx, (category, score) in enumerate(rankings, start=1):\n",
    "            print(f\"{idx}. {category}: {score:.2f}% at least somewhat familiar\")\n",
    "\n",
    "    # Calculate and print the average familiarity score for each category across all principles\n",
    "    avg_familiarity_scores = {\n",
    "        category: sum(scores) / len(scores) for category, scores in avg_familiarity_by_category.items()\n",
    "    }\n",
    "\n",
    "    # Sort the categories based on their average scores\n",
    "    sorted_avg_scores = sorted(avg_familiarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\nAverage familiarity across all principles for {demographic}:\")\n",
    "    for idx, (category, avg_score) in enumerate(sorted_avg_scores, start=1):\n",
    "        print(f\"{idx}. {category}: {avg_score:.2f}% average familiarity\")\n",
    "\n",
    "# Example usage:\n",
    "demographic_input = 'Location'  # Change to the desired demographic to analyze\n",
    "rank_by_familiarity_and_average(df, demographic_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Familiarity Heatmaps \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Build heatmaps of the regions to their familiarity of principles and regulations\n",
    "# Survey Questions (P8) -> (B.2.1 + A.1.1) combined and (P10) -> Regulations\n",
    "###################################################################\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('accepted_maybe_responses.csv')\n",
    "\n",
    "# Use the 28th column for the region (index 27 because Python uses 0-based indexing)\n",
    "region_column = df.columns[28]\n",
    "\n",
    "# Use columns CB through CJ (79:88) and AU through BC (46:55) for Likert scale responses (principles)\n",
    "likert_columns_1 = df.columns[79:88]  # CB through CJ\n",
    "likert_columns_2 = df.columns[46:55]  # AU through BC\n",
    "\n",
    "# Use columns HM through HT (220:228) for Likert scale responses (regulations)\n",
    "regulation_columns = df.columns[220:228]  # HM through HT\n",
    "\n",
    "# Create a mapping for the Likert scale responses to numeric values\n",
    "likert_mapping = {\n",
    "    \"Not Familiar At All\": 1,\n",
    "    \"Slightly Familiar\": 2,\n",
    "    \"Somewhat Familiar\": 3,\n",
    "    \"Moderately Familiar\": 4,\n",
    "    \"Extremely Familiar\": 5\n",
    "}\n",
    "\n",
    "# Create a mapping for the column names to the principles\n",
    "principle_mapping = {\n",
    "    df.columns[79]: \"Respect for Human Rights\",\n",
    "    df.columns[80]: \"Data Protection and Right to Privacy\",\n",
    "    df.columns[81]: \"Harm Prevention and Beneficence\",\n",
    "    df.columns[82]: \"Non-Discrimination and Freedom of Privileges\",\n",
    "    df.columns[83]: \"Fairness and Justice\",\n",
    "    df.columns[84]: \"Transparency and Explainability of AI Systems\",\n",
    "    df.columns[85]: \"Accountability and Responsibility\",\n",
    "    df.columns[86]: \"Democracy and Rule of Law\",\n",
    "    df.columns[87]: \"Environment and Social Responsibility\"\n",
    "}\n",
    "\n",
    "# Create a mapping for the column names to the regulations\n",
    "regulation_mapping = {\n",
    "    df.columns[220]: \"European Union Artificial Intelligence Act\",\n",
    "    df.columns[221]: \"US Executive Order on Safe, Secure and Trustworthy AI\",\n",
    "    df.columns[222]: \"US Algorithmic Accountability Act\",\n",
    "    df.columns[223]: \"NIST Technical AI Standards\",\n",
    "    df.columns[224]: \"NIST AI Risk Management Framework\",\n",
    "    df.columns[225]: \"UN General Assembly's Resolution on AI Systems\",\n",
    "    df.columns[226]: \"OECD Principles for Trustworthy AI\",\n",
    "    df.columns[227]: \"G20 AI Principles\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to all Likert scale columns\n",
    "for column in list(likert_columns_1) + list(likert_columns_2) + list(regulation_columns):\n",
    "    df[column] = df[column].map(likert_mapping)\n",
    "\n",
    "# Combine the two sets of Likert columns for principles\n",
    "for i in range(9):\n",
    "    combined_column = f'combined_principle_{i}'\n",
    "    df[combined_column] = df[[likert_columns_1[i], likert_columns_2[i]]].mean(axis=1)\n",
    "\n",
    "combined_principle_columns = [f'combined_principle_{i}' for i in range(9)]\n",
    "\n",
    "# Function to create and display heatmap\n",
    "def plot_heatmap(data, title, xlabel):\n",
    "    # Remove rows with all NaN values\n",
    "    data_clean = data.dropna(how='all')\n",
    "    \n",
    "    if data_clean.empty:\n",
    "        print(f\"No data to plot for: {title}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(20, 12))\n",
    "    sns.heatmap(data_clean, annot=True, cmap='coolwarm', linewidths=.5, fmt=\".2f\", cbar_kws={'label': 'Familiarity Level'})\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Region / Country')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate and plot for principles\n",
    "average_familiarity_by_region_principles = df.groupby(region_column)[combined_principle_columns].mean()\n",
    "average_familiarity_by_region_principles.columns = [principle_mapping[col] for col in likert_columns_1]\n",
    "plot_heatmap(average_familiarity_by_region_principles, 'Familiarity with AI Ethics Principles by Region (Combined)', 'AI Ethics Principles')\n",
    "\n",
    "# Calculate and plot for regulations\n",
    "average_familiarity_by_region_regulations = df.groupby(region_column)[regulation_columns].mean()\n",
    "average_familiarity_by_region_regulations = average_familiarity_by_region_regulations.rename(columns=regulation_mapping)\n",
    "plot_heatmap(average_familiarity_by_region_regulations, 'Familiarity with AI Regulations by Region', 'AI Regulations')\n",
    "\n",
    "# Create a new DataFrame with grouped regions\n",
    "def group_regions(region):\n",
    "    if region in ['North America', 'EU/UK/EEA']:\n",
    "        return region\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "grouped_df = df.copy()\n",
    "grouped_df[region_column] = grouped_df[region_column].apply(group_regions)\n",
    "\n",
    "# Calculate and plot for grouped principles\n",
    "grouped_average_familiarity_principles = grouped_df.groupby(region_column)[combined_principle_columns].mean()\n",
    "grouped_average_familiarity_principles.columns = [principle_mapping[col] for col in likert_columns_1]\n",
    "plot_heatmap(grouped_average_familiarity_principles, 'Familiarity with AI Ethics Principles by Grouped Regions (Combined)', 'AI Ethics Principles')\n",
    "\n",
    "# Calculate and plot for grouped regulations\n",
    "grouped_average_familiarity_regulations = grouped_df.groupby(region_column)[regulation_columns].mean()\n",
    "grouped_average_familiarity_regulations = grouped_average_familiarity_regulations.rename(columns=regulation_mapping)\n",
    "plot_heatmap(grouped_average_familiarity_regulations, 'Familiarity with AI Regulations by Grouped Regions', 'AI Regulations')\n",
    "\n",
    "# Save the data to CSV for further review\n",
    "average_familiarity_by_region_principles.to_csv('familiarity_by_region_and_principle_combined.csv')\n",
    "average_familiarity_by_region_regulations.to_csv('familiarity_by_region_and_regulation.csv')\n",
    "grouped_average_familiarity_principles.to_csv('familiarity_by_grouped_region_and_principle_combined.csv')\n",
    "grouped_average_familiarity_regulations.to_csv('familiarity_by_grouped_region_and_regulation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Build heatmaps of the roles to their familiarity with principles and regulations\n",
    "# Survey Questions (P9) -> (B.2.1 + A.1.1) and (P10) -> Regulations\n",
    "####################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('accepted_maybe_responses.csv')\n",
    "\n",
    "# Use the 31st column for the role (index 30 because Python uses 0-based indexing)\n",
    "role_column = df.columns[30]\n",
    "\n",
    "# Use columns CB through CJ (79:88) and AU through BC (46:55) for Likert scale responses (principles)\n",
    "likert_columns_1 = df.columns[79:88]  # CB through CJ\n",
    "likert_columns_2 = df.columns[46:55]  # AU through BC\n",
    "\n",
    "# Use columns HM through HT (213:221) for Likert scale responses (regulations)\n",
    "regulation_columns = df.columns[220:228]  # HM through HT\n",
    "\n",
    "# Create a mapping for the Likert scale responses to numeric values\n",
    "likert_mapping = {\n",
    "    \"Not Familiar At All\": 1,\n",
    "    \"Slightly Familiar\": 2,\n",
    "    \"Somewhat Familiar\": 3,\n",
    "    \"Moderately Familiar\": 4,\n",
    "    \"Extremely Familiar\": 5\n",
    "}\n",
    "\n",
    "# Create a mapping for the column names to the principles\n",
    "principle_mapping = {\n",
    "    0: \"Respect for Human Rights\",\n",
    "    1: \"Data Protection and Right to Privacy\",\n",
    "    2: \"Harm Prevention and Beneficence\",\n",
    "    3: \"Non-Discrimination and Freedom of Privileges\",\n",
    "    4: \"Fairness and Justice\",\n",
    "    5: \"Transparency and Explainability of AI Systems\",\n",
    "    6: \"Accountability and Responsibility\",\n",
    "    7: \"Democracy and Rule of Law\",\n",
    "    8: \"Environment and Social Responsibility\"\n",
    "}\n",
    "\n",
    "# Create a mapping for the column names to the regulations\n",
    "regulation_mapping = {\n",
    "    df.columns[220]: \"European Union Artificial Intelligence Act\",\n",
    "    df.columns[221]: \"US Executive Order on Safe, Secure and Trustworthy AI\",\n",
    "    df.columns[222]: \"US Algorithmic Accountability Act\",\n",
    "    df.columns[223]: \"NIST Technical AI Standards\",\n",
    "    df.columns[224]: \"NIST AI Risk Management Framework\",\n",
    "    df.columns[225]: \"UN General Assembly's Resolution on AI Systems\",\n",
    "    df.columns[226]: \"OECD Principles for Trustworthy AI\",\n",
    "    df.columns[227]: \"G20 AI Principles\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to all Likert scale columns\n",
    "for column in list(likert_columns_1) + list(likert_columns_2) + list(regulation_columns):\n",
    "    df[column] = df[column].map(likert_mapping)\n",
    "\n",
    "# Drop rows where the role is empty\n",
    "df = df.dropna(subset=[role_column])\n",
    "\n",
    "# Calculate the average of both sets of responses for principles\n",
    "for i in range(9):\n",
    "    df[f'combined_principle_{i}'] = df[[likert_columns_1[i], likert_columns_2[i]]].mean(axis=1)\n",
    "\n",
    "# Calculate the mean familiarity for each role and principle\n",
    "combined_principle_columns = [f'combined_principle_{i}' for i in range(9)]\n",
    "average_familiarity_by_role_principles = df.groupby(role_column)[combined_principle_columns].mean()\n",
    "\n",
    "# Rename the columns to the principles\n",
    "average_familiarity_by_role_principles.columns = [principle_mapping[i] for i in range(9)]\n",
    "\n",
    "# Calculate the mean familiarity for each role and regulation\n",
    "average_familiarity_by_role_regulations = df.groupby(role_column)[regulation_columns].mean()\n",
    "\n",
    "# Rename the columns to the regulations\n",
    "average_familiarity_by_role_regulations = average_familiarity_by_role_regulations.rename(columns=regulation_mapping)\n",
    "\n",
    "# Function to plot heatmap\n",
    "def plot_heatmap(data, title, ylabel):\n",
    "    if data.empty:\n",
    "        print(f\"No valid data to plot for {title}. Please check your input data.\")\n",
    "    else:\n",
    "        # Remove any rows or columns that are all NaN\n",
    "        data = data.dropna(how='all').dropna(axis=1, how='all')\n",
    "        \n",
    "        if data.empty:\n",
    "            print(f\"After removing NaN values, no data remains for {title}. Please check your input data.\")\n",
    "        else:\n",
    "            plt.figure(figsize=(20, 12))  # Increased figure size\n",
    "            sns.heatmap(data, annot=True, cmap='coolwarm', linewidths=.5, fmt=\".2f\", cbar_kws={'label': 'Average Familiarity Level'})\n",
    "            plt.title(title)\n",
    "            plt.xlabel('AI Ethics Principles / Regulations')\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Plot heatmap for principles\n",
    "plot_heatmap(average_familiarity_by_role_principles, 'Average Familiarity with AI Ethics Principles by Role (Combined Responses)', 'Role')\n",
    "\n",
    "# Plot heatmap for regulations\n",
    "plot_heatmap(average_familiarity_by_role_regulations, 'Average Familiarity with AI Regulations by Role', 'Role')\n",
    "\n",
    "# Save the pivoted data to CSV for further review\n",
    "average_familiarity_by_role_principles.to_csv('familiarity_by_role_and_principle_combined.csv')\n",
    "average_familiarity_by_role_regulations.to_csv('familiarity_by_role_and_regulation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
