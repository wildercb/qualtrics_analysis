{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Defenitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import spearmanr, chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from globals import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################################\n",
    "#### Is there a correlation between AI Managers Familiarity with ethical principles of AI and effectiveness of integrating principles? (B.9.1)/(H1a)\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Is there a correlation between AI Managers familiarity\n",
    "# with principles and effectiveness of principle integration? (B.9.1) / (H1a)\n",
    "#######################################################\n",
    "\n",
    "# Define file path\n",
    "file_path = survey_data\n",
    "\n",
    "def read_and_clean_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Successfully read {file_path}.\")\n",
    "        return df\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read {file_path} with utf-8 encoding.\")\n",
    "        return None\n",
    "\n",
    "# Read the CSV file\n",
    "df = read_and_clean_csv(file_path)\n",
    "\n",
    "if df is not None:\n",
    "    # Define the column index for the effectiveness question and role column\n",
    "    effectiveness_column = 'B.9.1'\n",
    "    role_column_index = demographics['Role']['column']\n",
    "    \n",
    "    # Step 1: Filter data for AI Managers\n",
    "    df['GroupedRole'] = df.iloc[:, role_column_index].map(demographics['Role']['mapping'])\n",
    "    ai_manager_df = df[df['GroupedRole'] == 'AI Manager'].copy()\n",
    "\n",
    "    # Step 2: Encode the effectiveness text responses safely\n",
    "    le = LabelEncoder()  # Create an instance of LabelEncoder\n",
    "    # Drop any NaNs in the effectiveness column before encoding\n",
    "    ai_manager_df = ai_manager_df.dropna(subset=[effectiveness_column])\n",
    "    ai_manager_df.loc[:, 'Effectiveness_Score'] = le.fit_transform(ai_manager_df[effectiveness_column].astype(str))\n",
    "\n",
    "    # Step 3: Calculate familiarity scores for Tracks A and B\n",
    "    familiarity_scores = []\n",
    "\n",
    "    for track in ['A', 'B']:\n",
    "        # Check that the column indices in principle_columns exist in the DataFrame\n",
    "        valid_columns = [col for col in principle_columns[track] if col < ai_manager_df.shape[1]]\n",
    "        valid_column_names = ai_manager_df.columns[valid_columns]\n",
    "\n",
    "        # Calculate the familiarity score for each row by averaging all principle responses\n",
    "        # NOTE THIS IS FOR AVERAGE FAMILIARITY OF ALL PRINCIPLES \n",
    "        track_familiarity_scores = ai_manager_df[valid_column_names].applymap(\n",
    "            lambda level: familiarity_levels.index(level) + 1 if level in familiarity_levels else np.nan\n",
    "        )\n",
    "        avg_familiarity_scores = track_familiarity_scores.mean(axis=1, skipna=True)\n",
    "        \n",
    "        familiarity_scores.append(avg_familiarity_scores)\n",
    "\n",
    "    # Combine familiarity scores across both tracks (A and B) into one series\n",
    "    combined_familiarity_scores = pd.concat(familiarity_scores, axis=0)\n",
    "\n",
    "    # Step 4: Align effectiveness scores to match combined familiarity scores and filter out NaNs\n",
    "    aligned_effectiveness_scores = ai_manager_df['Effectiveness_Score'].reindex(combined_familiarity_scores.index)\n",
    "    combined_data = pd.concat([combined_familiarity_scores, aligned_effectiveness_scores], axis=1).dropna()\n",
    "    combined_familiarity_scores_cleaned = combined_data.iloc[:, 0]\n",
    "    effectiveness_scores_cleaned = combined_data.iloc[:, 1]\n",
    "\n",
    "    # Step 5: Compute correlation\n",
    "    print(combined_familiarity_scores_cleaned, effectiveness_scores_cleaned)\n",
    "    correlation, p_value = spearmanr(combined_familiarity_scores_cleaned, effectiveness_scores_cleaned)\n",
    "\n",
    "    # Output the results\n",
    "    print(f\"Correlation between AI Manager familiarity and effectiveness (B.9.1): {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################\n",
    "#####  Is there a correlations between Requirements Analysts' familiarity with principles and:\n",
    "##### 1. frequency of including ethics in documentation (B.10.1) / (H1b)\n",
    "##### 2. number of principles considered in requirements (B.10.3) / (H1c)\n",
    "##### 3. impact of Ethics on AI lifecycle (B.10.2)\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Correlations between Requirements Analysts' familiarity\n",
    "# with principles and:\n",
    "# 1. frequency of including ethics in documentation (B.10.1) / (H1b)\n",
    "# 2. number of principles considered in requirements (B.10.3) / (H1c)\n",
    "#######################################################\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(survey_data)\n",
    "print(\"Successfully read AI_Study_Accepted.csv.\")\n",
    "\n",
    "# Filter for Requirements Analysts\n",
    "role_column = df.columns[30]\n",
    "req_analysts = df[df[role_column].isin(['Requirements Analyst or Engineer', 'Scrum Master, Product Manager, or Project Manager'])]\n",
    "# Calculate average familiarity score for Track B (columns 79-87)\n",
    "familiarity_columns = df.columns[79:88]\n",
    "for col in familiarity_columns:\n",
    "    req_analysts[col] = req_analysts[col].map(familiarity_mapping)\n",
    "req_analysts['Avg_Familiarity'] = req_analysts[familiarity_columns].mean(axis=1)\n",
    "\n",
    "# Get frequency scores (B.10.1)\n",
    "frequency_column = 'B.10.1'\n",
    "frequency_mapping = {'Never': 0, 'Rarely': 1, 'Sometimes': 2, 'Often': 3, 'Always': 4}\n",
    "req_analysts['Frequency_Score'] = req_analysts[frequency_column].map(frequency_mapping)\n",
    "\n",
    "# Count principles considered in requirements (B.10.3)\n",
    "principles_column = 'B.10.3'\n",
    "req_analysts['Principles_Count'] = req_analysts[principles_column].fillna('').apply(lambda x: len(x.split(',')) if x else 0)\n",
    "\n",
    "# Prepare data for correlations\n",
    "correlation_data = req_analysts[['Avg_Familiarity', 'Frequency_Score', 'Principles_Count']].dropna()\n",
    "\n",
    "print(f\"\\nNumber of Requirements Analysts: {len(req_analysts)}\")\n",
    "print(f\"Number of valid pairs for correlation: {len(correlation_data)}\")\n",
    "\n",
    "# Compute correlations if there's enough data\n",
    "if len(correlation_data) > 1:\n",
    "    corr_familiarity_frequency, p_value_freq = spearmanr(correlation_data['Avg_Familiarity'], correlation_data['Frequency_Score'])\n",
    "    corr_familiarity_principles, p_value_princ = spearmanr(correlation_data['Avg_Familiarity'], correlation_data['Principles_Count'])\n",
    "    \n",
    "    print(f\"\\nCorrelation between familiarity and frequency (B.10.1): {corr_familiarity_frequency:.4f}\")\n",
    "    print(f\"P-value: {p_value_freq:.4f}\")\n",
    "    \n",
    "    print(f\"\\nCorrelation between familiarity and number of principles considered (B.10.3): {corr_familiarity_principles:.4f}\")\n",
    "    print(f\"P-value: {p_value_princ:.4f}\")\n",
    "else:\n",
    "    print(\"\\nInsufficient data to compute correlations.\")\n",
    "\n",
    "# Print distributions\n",
    "print(\"\\nFamiliarity scores distribution:\")\n",
    "print(req_analysts['Avg_Familiarity'].value_counts(bins=5, sort=False))\n",
    "\n",
    "print(\"\\nFrequency scores distribution:\")\n",
    "print(req_analysts['Frequency_Score'].value_counts(sort=False))\n",
    "\n",
    "print(\"\\nNumber of principles considered distribution:\")\n",
    "print(req_analysts['Principles_Count'].value_counts(sort=False))\n",
    "\n",
    "print(\"\\nUnique responses in frequency column (B.10.1):\")\n",
    "print(req_analysts[frequency_column].unique())\n",
    "\n",
    "print(\"\\nSample of principles considered (B.10.3):\")\n",
    "print(req_analysts[principles_column].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Is there a correlation between Requirements analyst familiarity\n",
    "# with principles and impact of Ethics on AI lifecycle? (B.10.2)\n",
    "#######################################################\n",
    "file_path = survey_data\n",
    "\n",
    "def read_and_clean_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Successfully read {file_path}.\")\n",
    "        return df\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read {file_path} with utf-8 encoding.\")\n",
    "        return None\n",
    "\n",
    "# Read the CSV file\n",
    "df = read_and_clean_csv(file_path)\n",
    "\n",
    "if df is not None:\n",
    "    # Define the column index for the impact question and role column\n",
    "    impact_column = 'B.10.2'\n",
    "    role_column_index = demographics['Role']['column']\n",
    "    \n",
    "    # Step 1: Filter data for \"Requirements Analysts\"\n",
    "    df['GroupedRole'] = df.iloc[:, role_column_index].map(demographics['Role']['mapping'])\n",
    "    req_analyst_df = df[df['GroupedRole'] == 'Requirements analyst'].copy()\n",
    "\n",
    "    # Step 2: Drop rows where the impact response is \"Prefer not to say\"\n",
    "    req_analyst_df = req_analyst_df[req_analyst_df[impact_column] != 'Prefer not to say']\n",
    "\n",
    "    # Step 3: Encode the impact text responses safely\n",
    "    le = LabelEncoder()  # Create an instance of LabelEncoder\n",
    "    req_analyst_df.loc[:, 'Impact_Score'] = le.fit_transform(req_analyst_df[impact_column].astype(str))\n",
    "\n",
    "    # Step 4: Calculate familiarity scores for Tracks A and B\n",
    "    familiarity_scores = []\n",
    "\n",
    "    for track in ['A', 'B']:\n",
    "        # Check that the column indices in principle_columns exist in the DataFrame\n",
    "        valid_columns = [col for col in principle_columns[track] if col < req_analyst_df.shape[1]]\n",
    "        valid_column_names = req_analyst_df.columns[valid_columns]\n",
    "\n",
    "        # Calculate the familiarity score for each row by averaging all principle responses\n",
    "        track_familiarity_scores = req_analyst_df[valid_column_names].applymap(\n",
    "            lambda level: familiarity_levels.index(level) + 1 if level in familiarity_levels else np.nan\n",
    "        )\n",
    "        avg_familiarity_scores = track_familiarity_scores.mean(axis=1, skipna=True)\n",
    "        \n",
    "        familiarity_scores.append(avg_familiarity_scores)\n",
    "\n",
    "    # Combine familiarity scores across both tracks (A and B) into one series\n",
    "    combined_familiarity_scores = pd.concat(familiarity_scores, axis=0)\n",
    "\n",
    "    # Step 5: Align impact scores to match combined familiarity scores and filter out NaNs\n",
    "    aligned_impact_scores = req_analyst_df['Impact_Score'].reindex(combined_familiarity_scores.index)\n",
    "    combined_data = pd.concat([combined_familiarity_scores, aligned_impact_scores], axis=1).dropna()\n",
    "    combined_familiarity_scores_cleaned = combined_data.iloc[:, 0]\n",
    "    impact_scores_cleaned = combined_data.iloc[:, 1]\n",
    "\n",
    "    # Step 6: Compute correlation\n",
    "    correlation, p_value = spearmanr(combined_familiarity_scores_cleaned, impact_scores_cleaned)\n",
    "\n",
    "    # Output the results\n",
    "    print(f\"Correlation between Requirements Analyst familiarity and project impact (B.10.2): {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################\n",
    "##### Correlation between AI Developers' familiarity with principles\n",
    "##### and importance of transparency & explainability (B.11.2) / H1d\n",
    "##### and importance of ethics considerations? (B.11.3) / H1e\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Correlation between AI Developers' familiarity with principles\n",
    "# and importance of transparency & explainability? (B.11.2) / h1d\n",
    "# and importance of ethics considerations? (B.11.3) / H1e\n",
    "#######################################################\n",
    "df = pd.read_csv(survey_data)\n",
    "\n",
    "role_mapping = {\n",
    "    'AI Engineer or Developer': 'AI Developer',\n",
    "    '(Software) Developer, Designer, or Architect': 'AI Developer',\n",
    "    'Data Scientist or Data Analyst': 'AI Developer'\n",
    "}\n",
    "\n",
    "role_column = df.columns[30]\n",
    "ai_developers = df[df[role_column].isin(role_mapping.keys())].copy()\n",
    "\n",
    "# -------------------------------------------\n",
    "# Calculate average familiarity (Track B)\n",
    "# -------------------------------------------\n",
    "familiarity_columns = df.columns[79:88]\n",
    "for col in familiarity_columns:\n",
    "    ai_developers[col] = ai_developers[col].map(familiarity_mapping)\n",
    "ai_developers['Avg_Familiarity'] = ai_developers[familiarity_columns].mean(axis=1)\n",
    "\n",
    "# -------------------------------------------\n",
    "# B.11.3 - Correlation: familiarity vs confidence (H1e)\n",
    "# -------------------------------------------\n",
    "confidence_column = 'B.11.3'\n",
    "ai_developers['Confidence_Score'] = ai_developers[confidence_column].map(confidence_mapping)\n",
    "\n",
    "correlation_data_113 = ai_developers[['Avg_Familiarity', 'Confidence_Score']].dropna()\n",
    "\n",
    "if len(correlation_data_113) > 1:\n",
    "    corr_113, p_113 = spearmanr(\n",
    "        correlation_data_113['Avg_Familiarity'],\n",
    "        correlation_data_113['Confidence_Score']\n",
    "    )\n",
    "    print(f\"\\n[B.11.3] Correlation between Avg_Familiarity and Confidence: {corr_113:.4f}\")\n",
    "    print(f\"[B.11.3] P-value: {p_113:.4f}\")\n",
    "else:\n",
    "    print(\"\\n[B.11.3] Insufficient data to compute correlation.\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# B.11.2 - Correlation: familiarity vs importance (H1d)\n",
    "# -------------------------------------------\n",
    "importance_column = 'B.11.2'\n",
    "ai_developers['Importance_Score'] = ai_developers[importance_column].map(importance_mapping)\n",
    "\n",
    "correlation_data_112 = ai_developers[['Avg_Familiarity', 'Importance_Score']].dropna()\n",
    "\n",
    "if len(correlation_data_112) > 1:\n",
    "    corr_112, p_112 = spearmanr(\n",
    "        correlation_data_112['Avg_Familiarity'],\n",
    "        correlation_data_112['Importance_Score']\n",
    "    )\n",
    "    print(f\"\\n[B.11.2] Correlation between Avg_Familiarity and Importance: {corr_112:.4f}\")\n",
    "    print(f\"[B.11.2] P-value: {p_112:.4f}\")\n",
    "else:\n",
    "    print(\"\\n[B.11.2] Insufficient data to compute correlation.\")\n",
    "\n",
    "\n",
    "print(\"\\nFamiliarity scores distribution:\")\n",
    "print(ai_developers['Avg_Familiarity'].value_counts(bins=5, sort=False))\n",
    "\n",
    "print(\"\\nConfidence scores distribution (B.11.3):\")\n",
    "print(ai_developers['Confidence_Score'].value_counts(sort=False))\n",
    "\n",
    "print(\"\\nImportance scores distribution (B.11.2):\")\n",
    "print(ai_developers['Importance_Score'].value_counts(sort=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################\n",
    "##### Correlation between QA roles' familiarity with principles\n",
    "##### and importance of ethical considerations (B.12.2)\n",
    "##### and confidence in addressing ethical considerations (B.12.3)\n",
    "#######################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Correlation between QA roles' familiarity with principles\n",
    "# and importance of ethical considerations (B.12.2) / H1f\n",
    "# and confidence in addressing ethical considerations (B.12.3) / H1g\n",
    "#######################################################\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(survey_data)\n",
    "\n",
    "# Define QA role mapping\n",
    "qa_role_mapping = {\n",
    "    '(Software) Quality Assurance Engineer or Tester': 'QA'\n",
    "}\n",
    "\n",
    "role_column = df.columns[30]\n",
    "qa_roles = df[df[role_column].isin(qa_role_mapping.keys())].copy()\n",
    "print(f\"Total QA roles found: {len(qa_roles)}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Calculate average familiarity score for Track B\n",
    "# -------------------------------------------------------\n",
    "familiarity_columns = df.columns[79:88]\n",
    "for col in familiarity_columns:\n",
    "    qa_roles[col] = qa_roles[col].map(familiarity_mapping)\n",
    "\n",
    "qa_roles['Avg_Familiarity'] = qa_roles[familiarity_columns].mean(axis=1)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# B.12.2 - QA Familiarity vs. Importance of Ethical Considerations H1f\n",
    "# -------------------------------------------------------\n",
    "importance_column = 'B.12.2'  # Adjust if your column name differs\n",
    "qa_roles['Importance_Score'] = qa_roles[importance_column].map(importance_mapping)\n",
    "\n",
    "correlation_data_12_2 = qa_roles[['Avg_Familiarity', 'Importance_Score']].dropna()\n",
    "print(f\"\\nNumber of valid pairs for B.12.2 correlation: {len(correlation_data_12_2)}\")\n",
    "\n",
    "if len(correlation_data_12_2) > 1:\n",
    "    corr_12_2, p_12_2 = spearmanr(\n",
    "        correlation_data_12_2['Avg_Familiarity'],\n",
    "        correlation_data_12_2['Importance_Score']\n",
    "    )\n",
    "    print(f\"Correlation (B.12.2): {corr_12_2:.4f} | P-value: {p_12_2:.4f}\")\n",
    "else:\n",
    "    print(\"Insufficient data to compute correlation (B.12.2).\")\n",
    "\n",
    "# Distributions for B.12.2\n",
    "print(\"\\nFamiliarity scores distribution (QA roles):\")\n",
    "print(qa_roles['Avg_Familiarity'].value_counts(bins=5, sort=False))\n",
    "\n",
    "print(\"\\nImportance scores distribution (B.12.2):\")\n",
    "print(qa_roles['Importance_Score'].value_counts(sort=False))\n",
    "\n",
    "# Crosstab for B.12.2\n",
    "print(\"\\nCrosstab of familiarity and importance (B.12.2):\")\n",
    "familiarity_bins_12_2 = pd.cut(\n",
    "    correlation_data_12_2['Avg_Familiarity'],\n",
    "    bins=5,\n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "crosstab_12_2 = pd.crosstab(familiarity_bins_12_2, correlation_data_12_2['Importance_Score'])\n",
    "print(crosstab_12_2)\n",
    "\n",
    "# Print unique scores & original responses for B.12.2\n",
    "print(\"\\nUnique importance scores in the data (B.12.2):\")\n",
    "print(correlation_data_12_2['Importance_Score'].unique())\n",
    "\n",
    "print(\"\\nOriginal responses for B.12.2:\")\n",
    "print(qa_roles[importance_column].value_counts())\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# B.12.3 - QA Familiarity vs. Confidence in Ethical Considerations (H1g)\n",
    "# -------------------------------------------------------\n",
    "confidence_column = 'B.12.3'  # Adjust if your column name differs\n",
    "qa_roles['Confidence_Score'] = qa_roles[confidence_column].map(confidence_mapping)\n",
    "\n",
    "correlation_data_12_3 = qa_roles[['Avg_Familiarity', 'Confidence_Score']].dropna()\n",
    "print(f\"\\nNumber of valid pairs for B.12.3 correlation: {len(correlation_data_12_3)}\")\n",
    "\n",
    "if len(correlation_data_12_3) > 1:\n",
    "    corr_12_3, p_12_3 = spearmanr(\n",
    "        correlation_data_12_3['Avg_Familiarity'],\n",
    "        correlation_data_12_3['Confidence_Score']\n",
    "    )\n",
    "    print(f\"Correlation (B.12.3): {corr_12_3:.4f} | P-value: {p_12_3:.4f}\")\n",
    "else:\n",
    "    print(\"Insufficient data to compute correlation (B.12.3).\")\n",
    "\n",
    "# Distributions for B.12.3\n",
    "print(\"\\nFamiliarity scores distribution (QA roles):\")\n",
    "print(qa_roles['Avg_Familiarity'].value_counts(bins=5, sort=False))\n",
    "\n",
    "print(\"\\nConfidence scores distribution (B.12.3):\")\n",
    "print(qa_roles['Confidence_Score'].value_counts(sort=False))\n",
    "\n",
    "\n",
    "# Crosstab for B.12.3\n",
    "print(\"\\nCrosstab of familiarity and confidence (B.12.3):\")\n",
    "familiarity_bins_12_3 = pd.cut(\n",
    "    correlation_data_12_3['Avg_Familiarity'],\n",
    "    bins=5,\n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "crosstab_12_3 = pd.crosstab(familiarity_bins_12_3, correlation_data_12_3['Confidence_Score'])\n",
    "print(crosstab_12_3)\n",
    "\n",
    "# Print unique scores & original responses for B.12.3\n",
    "print(\"\\nUnique confidence scores in the data (B.12.3):\")\n",
    "print(correlation_data_12_3['Confidence_Score'].unique())\n",
    "\n",
    "print(\"\\nOriginal responses for B.12.3:\")\n",
    "print(qa_roles[confidence_column].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################\n",
    "#### Is there a correlation between (demographic) and familiarity with AI ethics principles? (H2)\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 1. Read in the Data\n",
    "##############################################\n",
    "df = pd.read_csv(survey_data)\n",
    "\n",
    "##############################################\n",
    "# 3. Process Ethics Familiarity Columns by Track\n",
    "##############################################\n",
    "for track in principle_columns.keys():\n",
    "    cols = principle_columns[track]\n",
    "    # Ensure the column indices exist in the DataFrame\n",
    "    valid_cols = [col for col in cols if col < df.shape[1]]\n",
    "    # Get the actual column names from the indices\n",
    "    col_names = [df.columns[col] for col in valid_cols]\n",
    "    \n",
    "    # Map each ethics response to its numeric value\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].map(familiarity_mapping)\n",
    "    \n",
    "    # Compute the overall (average) ethics familiarity for this track\n",
    "    avg_col_name = f'Avg_Ethics_Familiarity_{track}'\n",
    "    df[avg_col_name] = df[col_names].mean(axis=1)\n",
    "    print(f\"Computed overall ethics familiarity for Track {track} in column '{avg_col_name}'.\")\n",
    "\n",
    "# For each demographic, create a \"clean\" (mapped) column and a numeric (factorized) column.\n",
    "for demo_name, demo_info in demographics.items():\n",
    "    col_index = demo_info['column']\n",
    "    if col_index >= df.shape[1]:\n",
    "        print(f\"Warning: Column index {col_index} for {demo_name} not found in DataFrame.\")\n",
    "        continue\n",
    "    col_name = df.columns[col_index]\n",
    "    clean_col = f\"{demo_name}_clean\"\n",
    "    num_col = f\"{demo_name}_num\"\n",
    "    \n",
    "    if demo_info['mapping'] is not None:\n",
    "        df[clean_col] = df[col_name].map(demo_info['mapping'])\n",
    "    else:\n",
    "        df[clean_col] = df[col_name]\n",
    "    \n",
    "    df[num_col], _ = pd.factorize(df[clean_col])\n",
    "\n",
    "##############################################\n",
    "# 5. Initialize List to Store Significant Results\n",
    "##############################################\n",
    "significant_results = []\n",
    "\n",
    "##############################################\n",
    "# 6. Statistical Tests for Ethics Familiarity vs. Demographics\n",
    "##############################################\n",
    "for track in ['A', 'B']:\n",
    "    avg_ethics_col = f'Avg_Ethics_Familiarity_{track}'\n",
    "    print(f\"\\n=== Ethics Familiarity (Track {track}) vs. Demographics ===\\n\")\n",
    "    \n",
    "    # Overall Ethics Familiarity vs. Demographics\n",
    "    for demo_name in demographics.keys():\n",
    "        num_col = f\"{demo_name}_num\"\n",
    "        clean_col = f\"{demo_name}_clean\"\n",
    "        test_data = df[[avg_ethics_col, num_col]].dropna()\n",
    "        if test_data.empty:\n",
    "            print(f\"Insufficient data for {demo_name} (overall ethics familiarity, Track {track}).\")\n",
    "            continue\n",
    "        \n",
    "        # Spearman correlation (overall ethics familiarity vs. demographic numeric)\n",
    "        sp_corr, sp_p = spearmanr(test_data[avg_ethics_col], test_data[num_col])\n",
    "        print(f\"{demo_name} (Overall Ethics): Spearman r = {sp_corr:.4f} (p = {sp_p:.4f})\")\n",
    "        if sp_p < 0.05:\n",
    "            significant_results.append({\n",
    "                'Track': track,\n",
    "                'Test': 'Overall Ethics vs Demographic',\n",
    "                'Demographic': demo_name,\n",
    "                'Metric': 'Spearman',\n",
    "                'Statistic': sp_corr,\n",
    "                'p_value': sp_p\n",
    "            })\n",
    "        \n",
    "        # Chi-Square test for overall ethics familiarity (rounded) vs. categorical demographic\n",
    "        contingency_table = pd.crosstab(df[clean_col], df[avg_ethics_col].round())\n",
    "        if contingency_table.size > 0:\n",
    "            chi2_stat, chi2_p, dof, expected = chi2_contingency(contingency_table)\n",
    "            print(f\"   Chi-Square Statistic = {chi2_stat:.4f} (p = {chi2_p:.4f})\")\n",
    "            if chi2_p < 0.05:\n",
    "                significant_results.append({\n",
    "                    'Track': track,\n",
    "                    'Test': 'Overall Ethics vs Demographic',\n",
    "                    'Demographic': demo_name,\n",
    "                    'Metric': 'Chi-Square',\n",
    "                    'Statistic': chi2_stat,\n",
    "                    'p_value': chi2_p\n",
    "                })\n",
    "        else:\n",
    "            print(\"   Insufficient data for Chi-Square test.\")\n",
    "        print()\n",
    "    \n",
    "    # Per-Principle Ethics Familiarity vs. Demographics\n",
    "    valid_cols = [col for col in principle_columns[track] if col < df.shape[1]]\n",
    "    col_names = [df.columns[col] for col in valid_cols]\n",
    "    \n",
    "    for i, col in enumerate(col_names):\n",
    "        principle_name = principles[i] if i < len(principles) else f\"Principle_{i}\"\n",
    "        print(f\"--- {principle_name} (Track {track}) ---\")\n",
    "        for demo_name in demographics.keys():\n",
    "            num_col = f\"{demo_name}_num\"\n",
    "            clean_col = f\"{demo_name}_clean\"\n",
    "            sub_data = df[[col, num_col]].dropna()\n",
    "            if sub_data.empty:\n",
    "                print(f\"{demo_name}: Insufficient data for {principle_name}.\")\n",
    "                continue\n",
    "            \n",
    "            # Spearman correlation for this principle vs. demographic\n",
    "            sp_corr, sp_p = spearmanr(sub_data[col], sub_data[num_col])\n",
    "            print(f\"{demo_name}: Spearman r = {sp_corr:.4f} (p = {sp_p:.4f})\", end=\"; \")\n",
    "            if sp_p < 0.05:\n",
    "                significant_results.append({\n",
    "                    'Track': track,\n",
    "                    'Test': 'Per-Principle Ethics vs Demographic',\n",
    "                    'Demographic': demo_name,\n",
    "                    'Principle': principle_name,\n",
    "                    'Metric': 'Spearman',\n",
    "                    'Statistic': sp_corr,\n",
    "                    'p_value': sp_p\n",
    "                })\n",
    "            \n",
    "            # Chi-Square test for this principle (categorical) vs. demographic (categorical)\n",
    "            contingency = pd.crosstab(df[clean_col], df[col])\n",
    "            if contingency.size > 0:\n",
    "                chi2_stat, chi2_p, dof, expected = chi2_contingency(contingency)\n",
    "                print(f\"Chi-Square = {chi2_stat:.4f} (p = {chi2_p:.4f})\")\n",
    "                if chi2_p < 0.05:\n",
    "                    significant_results.append({\n",
    "                        'Track': track,\n",
    "                        'Test': 'Per-Principle Ethics vs Demographic',\n",
    "                        'Demographic': demo_name,\n",
    "                        'Principle': principle_name,\n",
    "                        'Metric': 'Chi-Square',\n",
    "                        'Statistic': chi2_stat,\n",
    "                        'p_value': chi2_p\n",
    "                    })\n",
    "            else:\n",
    "                print(\"Insufficient data for Chi-Square test.\")\n",
    "        print()\n",
    "\n",
    "##############################################\n",
    "# 7. Print All Significant Results (p < 0.05)\n",
    "##############################################\n",
    "print(\"\\n=== Significant Results (p < 0.05) ===\\n\")\n",
    "if not significant_results:\n",
    "    print(\"No tests reached significance (p < 0.05).\")\n",
    "else:\n",
    "    for result in significant_results:\n",
    "        if result['Test'] == 'Overall Ethics vs Demographic':\n",
    "            desc = (f\"Track {result['Track']} - {result['Test']} ({result['Demographic']} - {result['Metric']}): \"\n",
    "                    f\"Statistic = {result['Statistic']:.4f}, p = {result['p_value']:.4f}\")\n",
    "        else:\n",
    "            desc = (f\"Track {result['Track']} - {result['Test']} ({result['Demographic']}, Principle: {result.get('Principle','N/A')} - {result['Metric']}): \"\n",
    "                    f\"Statistic = {result['Statistic']:.4f}, p = {result['p_value']:.4f}\")\n",
    "        print(desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################\n",
    "### Is there a correlation between (demographic) and familiarity with AI Governance Initiatives? (H3)\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# 1. Read and Prepare the Data\n",
    "############################################################\n",
    "df = pd.read_csv(survey_data)\n",
    "\n",
    "# Assume governance regulation familiarity columns are columns 220-228.\n",
    "familiarity_column_indices = list(range(220, 229))  # adjust if necessary\n",
    "valid_columns = [col for col in familiarity_column_indices if col < df.shape[1]]\n",
    "print(f\"Valid familiarity columns (by index): {valid_columns}\")\n",
    "\n",
    "# Convert indices to column names\n",
    "valid_column_names = [df.columns[col] for col in valid_columns]\n",
    "print(f\"Valid familiarity column names: {valid_column_names}\")\n",
    "\n",
    "if len(valid_column_names) < len(regulation_names):\n",
    "    regulation_names = regulation_names[:len(valid_column_names)]\n",
    "elif len(valid_column_names) > len(regulation_names):\n",
    "    regulation_names.extend(['Unnamed Regulation'] * (len(valid_column_names) - len(regulation_names)))\n",
    "\n",
    "# Map the familiarity responses to numeric scores for each regulation column.\n",
    "for col in valid_column_names:\n",
    "    df[col] = df[col].map(familiarity_mapping)\n",
    "\n",
    "# Calculate the average familiarity score across all governance regulation columns.\n",
    "df['Avg_Regulation_Familiarity'] = df[valid_column_names].mean(axis=1)\n",
    "\n",
    "# For each demographic, create a \"clean\" column (using mapping, if provided)\n",
    "# and a numeric version (using pd.factorize) for correlation tests.\n",
    "for demo_name, demo_info in demographics.items():\n",
    "    col_index = demo_info['column']\n",
    "    if col_index >= df.shape[1]:\n",
    "        print(f\"Warning: Column index {col_index} for {demo_name} not found!\")\n",
    "        continue\n",
    "    col_name = df.columns[col_index]\n",
    "    clean_col = f\"{demo_name}_clean\"\n",
    "    num_col = f\"{demo_name}_num\"\n",
    "    \n",
    "    if demo_info['mapping'] is not None:\n",
    "        df[clean_col] = df[col_name].map(demo_info['mapping'])\n",
    "    else:\n",
    "        df[clean_col] = df[col_name]\n",
    "    \n",
    "    df[num_col], _ = pd.factorize(df[clean_col])\n",
    "\n",
    "############################################################\n",
    "# 4. Overall Familiarity vs. Each Demographic\n",
    "############################################################\n",
    "print(\"\\n=== Overall Avg Regulation Familiarity vs. Demographics ===\\n\")\n",
    "for demo_name in demographics.keys():\n",
    "    num_col = f\"{demo_name}_num\"\n",
    "    clean_col = f\"{demo_name}_clean\"\n",
    "    \n",
    "    # Prepare data: drop any rows with missing values for the overall familiarity or the demographic.\n",
    "    overall_data = df[['Avg_Regulation_Familiarity', num_col]].dropna()\n",
    "    if overall_data.empty:\n",
    "        print(f\"Insufficient data for {demo_name}.\")\n",
    "        continue\n",
    "\n",
    "    # Spearman correlation using numeric values\n",
    "    overall_corr, overall_p = spearmanr(overall_data['Avg_Regulation_Familiarity'], overall_data[num_col])\n",
    "    print(f\"{demo_name}:\")\n",
    "    print(f\"  Spearman correlation (Avg_Regulation_Familiarity vs. {demo_name}): {overall_corr:.4f} (p = {overall_p:.4f})\")\n",
    "    \n",
    "    # For the Chi-Square test, create a contingency table. Here we round the overall familiarity \n",
    "    overall_contingency = pd.crosstab(df[clean_col], df['Avg_Regulation_Familiarity'].round())\n",
    "    if overall_contingency.size > 0:\n",
    "        chi2_stat, chi2_p, dof, expected = chi2_contingency(overall_contingency)\n",
    "        print(f\"  Chi-Square Statistic: {chi2_stat:.4f} (p = {chi2_p:.4f})\")\n",
    "    else:\n",
    "        print(\"  Insufficient data for Chi-Square test.\")\n",
    "    print()\n",
    "\n",
    "############################################################\n",
    "# 5. Per-Regulation Familiarity vs. Each Demographic (Optional)\n",
    "############################################################\n",
    "print(\"\\n=== Per-Regulation Familiarity vs. Demographics ===\\n\")\n",
    "for demo_name in demographics.keys():\n",
    "    num_col = f\"{demo_name}_num\"\n",
    "    clean_col = f\"{demo_name}_clean\"\n",
    "    print(f\"\\n--- Demographic: {demo_name} ---\")\n",
    "    \n",
    "    for i, reg_col in enumerate(valid_column_names):\n",
    "        reg_name = regulation_names[i]\n",
    "        print(f\"\\nRegulation: {reg_name}\")\n",
    "        \n",
    "        # Spearman correlation for the current regulation and the demographic.\n",
    "        reg_data = df[[reg_col, num_col]].dropna()\n",
    "        if not reg_data.empty:\n",
    "            reg_corr, reg_p = spearmanr(reg_data[reg_col], reg_data[num_col])\n",
    "            print(f\"  Spearman correlation: {reg_corr:.4f} (p = {reg_p:.4f})\")\n",
    "        else:\n",
    "            print(\"  Insufficient data for Spearman correlation.\")\n",
    "        \n",
    "        # Chi-Square test for the current regulation using the categorical demographic.\n",
    "        reg_contingency = pd.crosstab(df[clean_col], df[reg_col])\n",
    "        if reg_contingency.size > 0:\n",
    "            chi2_reg, chi2_reg_p, dof, expected = chi2_contingency(reg_contingency)\n",
    "            print(f\"  Chi-Square Statistic: {chi2_reg:.4f} (p = {chi2_reg_p:.4f})\")\n",
    "        else:\n",
    "            print(\"  Insufficient data for Chi-Square test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Correlation between QA roles' familiarity with principles\n",
    "# and importance of ethical considerations (B.12.2)\n",
    "#######################################################\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(survey_data)\n",
    "\n",
    "# Define QA role mapping\n",
    "qa_role_mapping = {\n",
    "    '(Software) Quality Assurance Engineer or Tester': 'QA'\n",
    "}\n",
    "\n",
    "# Filter for QA roles\n",
    "role_column = df.columns[30]\n",
    "qa_roles = df[df[role_column].isin(qa_role_mapping.keys())].copy()\n",
    "\n",
    "# Calculate average familiarity score for Track B (columns 79-87)\n",
    "familiarity_columns = df.columns[79:88]\n",
    "for col in familiarity_columns:\n",
    "    qa_roles[col] = qa_roles[col].map(familiarity_mapping)\n",
    "qa_roles['Avg_Familiarity'] = qa_roles[familiarity_columns].mean(axis=1)\n",
    "\n",
    "# Map importance scores (B.12.2)\n",
    "importance_column = 'B.12.2'\n",
    "qa_roles['Importance_Score'] = qa_roles[importance_column].map(importance_mapping)\n",
    "\n",
    "# Prepare data for correlation\n",
    "correlation_data = qa_roles[['Avg_Familiarity', 'Importance_Score']].dropna()\n",
    "\n",
    "print(f\"\\nNumber of QA roles: {len(qa_roles)}\")\n",
    "print(f\"Number of valid pairs for correlation: {len(correlation_data)}\")\n",
    "\n",
    "# Compute correlation if there's enough data\n",
    "if len(correlation_data) > 1:\n",
    "    correlation, p_value = spearmanr(correlation_data['Avg_Familiarity'], correlation_data['Importance_Score'])\n",
    "    print(f\"\\nCorrelation between QA roles' familiarity and importance of ethical considerations (B.12.2): {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "else:\n",
    "    print(\"\\nInsufficient data to compute correlation.\")\n",
    "\n",
    "# Print distributions\n",
    "print(\"\\nFamiliarity scores distribution:\")\n",
    "print(qa_roles['Avg_Familiarity'].value_counts(bins=5, sort=False))\n",
    "\n",
    "print(\"\\nImportance scores distribution:\")\n",
    "print(qa_roles['Importance_Score'].value_counts(sort=False))\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(correlation_data['Avg_Familiarity'], correlation_data['Importance_Score'])\n",
    "plt.title(\"QA Roles: Familiarity vs Importance of Ethical Considerations\")\n",
    "plt.xlabel(\"Average Familiarity with Principles\")\n",
    "plt.ylabel(\"Importance of Ethical Considerations\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Display a crosstab of familiarity and importance scores\n",
    "print(\"\\nCrosstab of familiarity and importance scores:\")\n",
    "familiarity_bins = pd.cut(correlation_data['Avg_Familiarity'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "crosstab = pd.crosstab(familiarity_bins, correlation_data['Importance_Score'])\n",
    "print(crosstab)\n",
    "\n",
    "# Print unique importance scores\n",
    "print(\"\\nUnique importance scores in the data:\")\n",
    "print(correlation_data['Importance_Score'].unique())\n",
    "\n",
    "# Print original responses for B.12.2\n",
    "print(\"\\nOriginal responses for B.12.2:\")\n",
    "print(qa_roles[importance_column].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Chi-Square Test between location (column 28) \n",
    "# and effectiveness in AI ethics integration (B.9.1)\n",
    "#######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Read the CSV file (assuming it's named 'AI_Study_Accepted.csv')\n",
    "df = pd.read_csv(survey_data)\n",
    "\n",
    "location_column = df.columns[28]\n",
    "\n",
    "# Map locations into groups\n",
    "df['Grouped_Location'] = df[location_column].map(location_mapping)\n",
    "\n",
    "# Use the B.9.1 column for effectiveness (replace with actual column name if it's different)\n",
    "effectiveness_column = 'B.9.1'  # Replace with the actual column name for B.9.1\n",
    "df['Effectiveness_Level'] = df[effectiveness_column].map(effectiveness_mapping)\n",
    "\n",
    "# Drop rows with missing data for effectiveness or location\n",
    "correlation_data = df[['Effectiveness_Level', 'Grouped_Location']].dropna()\n",
    "\n",
    "# Print data for debugging\n",
    "print(\"Unique values in 'Grouped_Location':\")\n",
    "print(correlation_data['Grouped_Location'].unique())\n",
    "\n",
    "print(\"\\nValue counts for 'Effectiveness_Level':\")\n",
    "print(correlation_data['Effectiveness_Level'].value_counts())\n",
    "\n",
    "# Perform the Chi-Square Test of Independence\n",
    "if correlation_data.empty:\n",
    "    print(\"No valid data available for Chi-Square test.\")\n",
    "else:\n",
    "    #######################\n",
    "    # Chi-Square Test\n",
    "    #######################\n",
    "    # Create a contingency table (cross-tabulation of the two variables)\n",
    "    contingency_table = pd.crosstab(correlation_data['Grouped_Location'], correlation_data['Effectiveness_Level'])\n",
    "\n",
    "    # Perform Chi-Square test\n",
    "    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Display the Chi-Square results\n",
    "    print(\"\\nContingency Table:\")\n",
    "    print(contingency_table)\n",
    "\n",
    "    print(f\"\\nChi-Square Statistic: {chi2_stat:.4f}\")\n",
    "    print(f\"Degrees of Freedom: {dof}\")\n",
    "    print(f\"P-Value: {p_val:.4f}\")\n",
    "\n",
    "    if p_val < 0.05:\n",
    "        print(\"\\nResult: There is a statistically significant association between location and effectiveness of AI ethics integration.\")\n",
    "    else:\n",
    "        print(\"\\nResult: There is no statistically significant association between location and effectiveness of AI ethics integration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Chi-Square and Spearman correlation for location (column 28)\n",
    "# and the selected principles from B.2.2 (Individual principle)\n",
    "#######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency, spearmanr\n",
    "\n",
    "# Read the CSV file (assuming it's named 'AI_Study_Accepted.csv')\n",
    "df = pd.read_csv('AI_Study_Accepted.csv')\n",
    "print(\"Successfully read AI_Study_Accepted.csv.\")\n",
    "\n",
    "# Define the principles in B.2.2\n",
    "principles = [\n",
    "    'Respect for Human Rights', 'Data Protection and Right to Privacy',\n",
    "    'Harm Prevention and Beneficence', 'Non-Discrimination and Freedom of Privileges',\n",
    "    'Fairness and Justice', 'Transparency and Explainability of AI Systems',\n",
    "    'Accountability and Responsibility', 'Democracy and Rule of Law',\n",
    "    'Environment and Social Responsibility'\n",
    "]\n",
    "\n",
    "# Define location grouping (use column 28)\n",
    "location_mapping = {\n",
    "    'North America': 'North America',\n",
    "    'EU/UK/EEA': 'Europe',\n",
    "    'Europe - Outside of EU/UK/EEA': 'Europe',\n",
    "    'Central/South America': 'Others',\n",
    "    'Africa': 'Others',\n",
    "    'Middle East': 'Others',\n",
    "    'Asia': 'Others',\n",
    "    'Australia and Oceania': 'Others'\n",
    "}\n",
    "\n",
    "# Use the correct location column (column 28, zero-indexed so it's 27)\n",
    "location_column = df.columns[28]  # Adjusted for zero-indexed column\n",
    "\n",
    "# Map locations into groups\n",
    "df['Grouped_Location'] = df[location_column].map(location_mapping)\n",
    "\n",
    "# Process the B.2.2 column for principles selection (replace 'B.2.2' with actual column name)\n",
    "principles_column = 'B.2.2'  # Replace with the actual column name for B.2.2\n",
    "df[principles_column] = df[principles_column].fillna('')\n",
    "\n",
    "# Create binary columns for each principle\n",
    "for principle in principles:\n",
    "    df[principle] = df[principles_column].apply(lambda x: 1 if (principle in x or 'All' in x) else 0)\n",
    "\n",
    "# Drop rows with missing location data\n",
    "correlation_data = df[['Grouped_Location'] + principles].dropna()\n",
    "\n",
    "# Print data for debugging\n",
    "print(\"Unique values in 'Grouped_Location':\")\n",
    "print(correlation_data['Grouped_Location'].unique())\n",
    "\n",
    "# Perform both Chi-Square and Spearman correlation for each principle\n",
    "for principle in principles:\n",
    "    print(f\"\\n### Analyzing principle: {principle} ###\")\n",
    "\n",
    "    #######################\n",
    "    # Chi-Square Test\n",
    "    #######################\n",
    "    contingency_table = pd.crosstab(correlation_data['Grouped_Location'], correlation_data[principle])\n",
    "\n",
    "    # Perform Chi-Square test\n",
    "    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    print(\"\\nContingency Table:\")\n",
    "    print(contingency_table)\n",
    "\n",
    "    print(f\"\\nChi-Square Statistic: {chi2_stat:.4f}\")\n",
    "    print(f\"Degrees of Freedom: {dof}\")\n",
    "    print(f\"P-Value: {p_val:.4f}\")\n",
    "\n",
    "    if p_val < 0.05:\n",
    "        print(f\"\\nResult: There is a statistically significant association between location and selection of '{principle}'.\")\n",
    "    else:\n",
    "        print(f\"\\nResult: There is no statistically significant association between location and selection of '{principle}'.\")\n",
    "\n",
    "    #######################\n",
    "    # Spearman Correlation\n",
    "    #######################\n",
    "    # Convert Grouped_Location to numeric categories for Spearman correlation\n",
    "    correlation_data['Location_Code'] = correlation_data['Grouped_Location'].astype('category').cat.codes\n",
    "\n",
    "    # Perform Spearman correlation between principle selection and location code\n",
    "    spearman_corr, spearman_p = spearmanr(correlation_data[principle], correlation_data['Location_Code'])\n",
    "\n",
    "    # Display Spearman correlation results\n",
    "    print(f\"\\nSpearman correlation for '{principle}': {spearman_corr:.4f}\")\n",
    "    print(f\"P-value: {spearman_p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Correlation between location (column 28)\n",
    "# and familiarity with AI governance regulations(individual)\n",
    "# Includes both Spearman and Chi-Square tests \n",
    "#######################################################\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(survey_data)\n",
    "\n",
    "\n",
    "# Define familiarity mapping for regulations\n",
    "familiarity_mapping = {\n",
    "    'Extremely Familiar': 5,\n",
    "    'Moderately Familiar': 4,\n",
    "    'Somewhat Familiar': 3,\n",
    "    'Slightly Familiar': 2,\n",
    "    'Not Familiar At All': 1\n",
    "}\n",
    "\n",
    "# Group locations\n",
    "def group_location(location):\n",
    "    if location in ['EU/UK/EEA', 'Europe - Outside of EU/UK/EEA']:\n",
    "        return 'Europe'\n",
    "    elif location == 'North America':\n",
    "        return 'North America'\n",
    "    else:\n",
    "        return 'Others'\n",
    "\n",
    "# Apply location grouping\n",
    "location_column = df.columns[28]  # Adjust column index if necessary\n",
    "df['Location_Group'] = df[location_column].apply(group_location)\n",
    "\n",
    "# Columns related to AI governance regulations (e.g., columns 220-228)\n",
    "familiarity_column_indices = list(range(220, 229))  # Check columns 220 to 228\n",
    "\n",
    "# Ensure that the columns actually exist in the DataFrame\n",
    "valid_columns = [col for col in familiarity_column_indices if col < df.shape[1]]\n",
    "print(f\"Valid columns are: {valid_columns}\")\n",
    "\n",
    "# Convert valid_columns (which are indices) into actual column names\n",
    "valid_column_names = [df.columns[col] for col in valid_columns]\n",
    "print(f\"Valid column names are: {valid_column_names}\")\n",
    "\n",
    "\n",
    "# Truncate or pad the regulation_names list to match the number of valid columns\n",
    "if len(valid_column_names) < len(regulation_names):\n",
    "    regulation_names = regulation_names[:len(valid_column_names)]\n",
    "elif len(valid_column_names) > len(regulation_names):\n",
    "    regulation_names.extend(['Unnamed Regulation'] * (len(valid_column_names) - len(regulation_names)))\n",
    "\n",
    "# Map familiarity levels to numerical scores for each valid column\n",
    "for col in valid_column_names:\n",
    "    df[col] = df[col].map(familiarity_mapping)\n",
    "\n",
    "# Calculate average familiarity score across all valid governance initiative columns\n",
    "df['Avg_Regulation_Familiarity'] = df[valid_column_names].mean(axis=1)\n",
    "\n",
    "# Map location groups to numerical values\n",
    "location_mapping = {'Europe': 0, 'North America': 1, 'Others': 2}\n",
    "df['Location_Numeric'] = df['Location_Group'].map(location_mapping)\n",
    "\n",
    "# Prepare data for correlation\n",
    "correlation_data = df[['Avg_Regulation_Familiarity', 'Location_Numeric']].dropna()\n",
    "print(f\"\\nNumber of valid pairs for correlation: {len(correlation_data)}\")\n",
    "\n",
    "# Compute Spearman correlation for overall familiarity\n",
    "correlation, p_value = spearmanr(correlation_data['Avg_Regulation_Familiarity'], correlation_data['Location_Numeric'])\n",
    "print(f\"\\nSpearman correlation between location and overall familiarity with AI governance regulations: {correlation:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Perform both Spearman and Chi-Square tests for each regulation\n",
    "print(\"\\n--- Spearman Correlation and Chi-Square Test for each regulation ---\\n\")\n",
    "\n",
    "for i, col in enumerate(valid_column_names):\n",
    "    print(f\"\\nAnalyzing regulation: {regulation_names[i]}\")\n",
    "\n",
    "    #######################\n",
    "    # Spearman Correlation\n",
    "    #######################\n",
    "    reg_correlation_data = df[[col, 'Location_Numeric']].dropna()\n",
    "    \n",
    "    if not reg_correlation_data.empty:\n",
    "        spearman_corr, spearman_p = spearmanr(reg_correlation_data[col], reg_correlation_data['Location_Numeric'])\n",
    "        print(f\"Spearman correlation: {spearman_corr:.4f}\")\n",
    "        print(f\"P-value: {spearman_p:.4f}\")\n",
    "    else:\n",
    "        print(\"Insufficient data for Spearman correlation.\")\n",
    "\n",
    "    #######################\n",
    "    # Chi-Square Test\n",
    "    #######################\n",
    "    # Create a contingency table for Chi-Square test\n",
    "    contingency_table = pd.crosstab(df['Location_Group'], df[col])\n",
    "\n",
    "    if contingency_table.size > 0:\n",
    "        chi2_stat, chi_p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "        print(f\"Chi-Square Statistic: {chi2_stat:.4f}\")\n",
    "        print(f\"P-value: {chi_p_val:.4f}\")\n",
    "    else:\n",
    "        print(\"Insufficient data for Chi-Square test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Is there a correlation between primary applications and principles considere in development?\n",
    "######################################\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(survey_data)\n",
    "\n",
    "# Define columns for applications and ethics principles (assuming they are named appropriately)\n",
    "application_column = \"B.1.2\"  # Column with all applications selected\n",
    "principle_column = \"B.2.2\"    # Column with all principles selected\n",
    "\n",
    "# Create an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Step 1: Expand the applications column\n",
    "df_app_expanded = df[application_column].str.split(',').explode().reset_index(drop=True)\n",
    "df_principle_expanded = df[principle_column].str.split(',').explode().reset_index(drop=True)\n",
    "\n",
    "# Combine exploded data into a new DataFrame\n",
    "combined_df = pd.DataFrame({\n",
    "    \"Application\": df_app_expanded.str.strip(),  # Strip spaces around values\n",
    "    \"Principle\": df_principle_expanded.str.strip()  # Strip spaces around values\n",
    "}).dropna()  # Drop rows where either Application or Principle is NaN\n",
    "\n",
    "# Step 2: Identify unique applications and principles\n",
    "unique_applications = combined_df[\"Application\"].unique()\n",
    "unique_principles = combined_df[\"Principle\"].unique()\n",
    "\n",
    "# Step 3: Run chi-square tests for each application-principle pair\n",
    "for app in unique_applications:\n",
    "    for principle in unique_principles:\n",
    "        # Filter for the current application and principle to create a contingency table\n",
    "        contingency_table = pd.crosstab(\n",
    "            combined_df[\"Application\"] == app,\n",
    "            combined_df[\"Principle\"] == principle\n",
    "        )\n",
    "        \n",
    "        # Only run chi-square if we have a valid table (2x2 or greater)\n",
    "        if contingency_table.shape[0] > 1 and contingency_table.shape[1] > 1:\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            results.append({\n",
    "                \"Application\": app,\n",
    "                \"Principle\": principle,\n",
    "                \"Chi2\": chi2,\n",
    "                \"p-value\": p\n",
    "            })\n",
    "            print(f\"Chi-Square Test for {app} and {principle}\")\n",
    "            print(f\"Chi2: {chi2:.4f}, p-value: {p:.4f}\")\n",
    "            if p < 0.05:\n",
    "                print(\"Significant association found!\")\n",
    "            else:\n",
    "                print(\"No significant association.\")\n",
    "        else:\n",
    "            print(f\"Insufficient data for Chi-Square test on '{app}' and '{principle}'.\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# Filter for significant results only\n",
    "significant_results = result_df[result_df[\"p-value\"] < 0.05]\n",
    "\n",
    "# Step 4: Create a pivot table for visualization\n",
    "pivot_table = significant_results.pivot(index=\"Application\", columns=\"Principle\", values=\"Chi2\")\n",
    "\n",
    "# Step 5: Plot heatmap for significant results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_table, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar=True)\n",
    "plt.title(\"Significant Associations Between AI Applications and Ethics Principles (Chi2)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Is there a correlation between ethical principles considered and applications of AI chosen \n",
    "###############################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Load the data with proper handling for quoted commas\n",
    "df = pd.read_csv(\"AI_Study_Accepted.csv\", quotechar='\"')\n",
    "print(\"Successfully loaded data without skipping rows.\")\n",
    "\n",
    "# Define the column names upfront based on the dataset\n",
    "application_column = \"B.1.2\"  # Primary applications of AI\n",
    "principle_column = \"B.2.2\"    # Ethics principles considered\n",
    "\n",
    "# Drop any non-data rows (adjust according to your actual CSV structure)\n",
    "df = df.drop([0, 1]).reset_index(drop=True)\n",
    "\n",
    "# Verify that the columns are still accessible\n",
    "if application_column not in df.columns or principle_column not in df.columns:\n",
    "    raise KeyError(f\"Columns '{application_column}' or '{principle_column}' not found in the data.\")\n",
    "\n",
    "# List of known application options to match exactly\n",
    "known_applications = [\n",
    "    \"Chatbots, Personal Assistants or Recommender Systems\",\n",
    "    \"Computer Vision\",\n",
    "    \"Customer Service\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Entertainment or Communication\",\n",
    "    \"Financial Services\",\n",
    "    \"Healthcare\",\n",
    "    \"Human Resources\",\n",
    "    \"Legal\",\n",
    "    \"Logistics\",\n",
    "    \"Personalization and Advertisement\",\n",
    "    \"Predictive Analysis\",\n",
    "    \"Programming Analysis (e.g., Code Completion or Code Generation)\",\n",
    "    \"Retail\",\n",
    "    \"Robotics and Automation\",\n",
    "    \"Translation or Text Generation\",\n",
    "    \"Prefer not to say\",\n",
    "    \"Other, please explain\"\n",
    "]\n",
    "\n",
    "# Compile a regex pattern to match any of these known application options exactly\n",
    "pattern = r'\\b(?:' + '|'.join(re.escape(option) for option in known_applications) + r')\\b'\n",
    "\n",
    "# Step 1: Extract applications by finding exact matches for known applications\n",
    "df_app_expanded = df[application_column].str.findall(pattern).explode().reset_index(drop=True)\n",
    "\n",
    "# Step 2: Expand the principles by splitting on commas\n",
    "df_principle_expanded = df[principle_column].str.split(',').explode().reset_index(drop=True)\n",
    "\n",
    "# Step 3: Combine the expanded columns into a single DataFrame for analysis\n",
    "combined_df = pd.DataFrame({\n",
    "    \"Application\": df_app_expanded.str.strip(),  # Strip any extra whitespace\n",
    "    \"Principle\": df_principle_expanded.str.strip()\n",
    "}).dropna()  # Remove any rows where either Application or Principle is NaN\n",
    "\n",
    "# Step 4: Get unique applications and principles\n",
    "unique_applications = combined_df[\"Application\"].unique()\n",
    "unique_principles = combined_df[\"Principle\"].unique()\n",
    "\n",
    "# Step 5: Run chi-square tests for each application-principle pair\n",
    "results = []\n",
    "for app in unique_applications:\n",
    "    for principle in unique_principles:\n",
    "        # Create contingency table for chi-square test\n",
    "        contingency_table = pd.crosstab(\n",
    "            combined_df[\"Application\"] == app,\n",
    "            combined_df[\"Principle\"] == principle\n",
    "        )\n",
    "        \n",
    "        # Only run chi-square if the table is at least 2x2\n",
    "        if contingency_table.shape[0] > 1 and contingency_table.shape[1] > 1:\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            results.append({\n",
    "                \"Application\": app,\n",
    "                \"Principle\": principle,\n",
    "                \"Chi2\": chi2,\n",
    "                \"p-value\": p\n",
    "            })\n",
    "            print(f\"Chi-Square Test for {app} and {principle}\")\n",
    "            print(f\"Chi2: {chi2:.4f}, p-value: {p:.4f}\")\n",
    "            if p < 0.05:\n",
    "                print(\"Significant association found!\")\n",
    "            else:\n",
    "                print(\"No significant association.\")\n",
    "        else:\n",
    "            print(f\"Insufficient data for Chi-Square test on '{app}' and '{principle}'.\")\n",
    "\n",
    "# Convert results to a DataFrame for visualization\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# Filter for significant results\n",
    "significant_results = result_df[result_df[\"p-value\"] < 0.05]\n",
    "\n",
    "# Create a pivot table and heatmap for significant results\n",
    "pivot_table = significant_results.pivot(index=\"Application\", columns=\"Principle\", values=\"Chi2\")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_table, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar=True)\n",
    "plt.title(\"Significant Associations Between AI Applications and Ethics Principles (Chi2)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Correlation between ethical principles seen at risk and applications of AI \n",
    "######################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Load the data with proper handling for quoted commas\n",
    "df = pd.read_csv(\"AI_Study_Accepted.csv\", quotechar='\"')\n",
    "print(\"Successfully loaded data without skipping rows.\")\n",
    "\n",
    "# Define the column names upfront based on the dataset\n",
    "application_column = \"B.1.2\"  # Primary applications of AI\n",
    "at_risk_principle_column = \"B.2.4\"  # Principles seen at risk\n",
    "\n",
    "# Drop any non-data rows (adjust according to your actual CSV structure)\n",
    "df = df.drop([0, 1]).reset_index(drop=True)\n",
    "\n",
    "# Verify that the columns are still accessible\n",
    "if application_column not in df.columns or at_risk_principle_column not in df.columns:\n",
    "    raise KeyError(f\"Columns '{application_column}' or '{at_risk_principle_column}' not found in the data.\")\n",
    "\n",
    "# List of known application options to match exactly\n",
    "known_applications = [\n",
    "    \"Chatbots, Personal Assistants or Recommender Systems\",\n",
    "    \"Computer Vision\",\n",
    "    \"Customer Service\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Entertainment or Communication\",\n",
    "    \"Financial Services\",\n",
    "    \"Healthcare\",\n",
    "    \"Human Resources\",\n",
    "    \"Legal\",\n",
    "    \"Logistics\",\n",
    "    \"Personalization and Advertisement\",\n",
    "    \"Predictive Analysis\",\n",
    "    \"Programming Analysis (e.g., Code Completion or Code Generation)\",\n",
    "    \"Retail\",\n",
    "    \"Robotics and Automation\",\n",
    "    \"Translation or Text Generation\",\n",
    "    \"Prefer not to say\",\n",
    "    \"Other, please explain\"\n",
    "]\n",
    "\n",
    "# Compile a regex pattern to match any of these known application options exactly\n",
    "pattern = r'\\b(?:' + '|'.join(re.escape(option) for option in known_applications) + r')\\b'\n",
    "\n",
    "# Step 1: Extract applications by finding exact matches for known applications\n",
    "df_app_expanded = df[application_column].str.findall(pattern).explode().reset_index(drop=True)\n",
    "\n",
    "# Step 2: Expand the principles seen at risk by splitting on commas\n",
    "df_at_risk_principle_expanded = df[at_risk_principle_column].str.split(',').explode().reset_index(drop=True)\n",
    "\n",
    "# Step 3: Combine the expanded columns into a single DataFrame for analysis\n",
    "combined_df = pd.DataFrame({\n",
    "    \"Application\": df_app_expanded.str.strip(),  # Strip any extra whitespace\n",
    "    \"At_Risk_Principle\": df_at_risk_principle_expanded.str.strip()\n",
    "}).dropna()  # Remove any rows where either Application or At_Risk_Principle is NaN\n",
    "\n",
    "# Step 4: Get unique applications and at-risk principles\n",
    "unique_applications = combined_df[\"Application\"].unique()\n",
    "unique_at_risk_principles = combined_df[\"At_Risk_Principle\"].unique()\n",
    "\n",
    "# Step 5: Run chi-square tests for each application-at-risk principle pair\n",
    "results = []\n",
    "for app in unique_applications:\n",
    "    for principle in unique_at_risk_principles:\n",
    "        # Create contingency table for chi-square test\n",
    "        contingency_table = pd.crosstab(\n",
    "            combined_df[\"Application\"] == app,\n",
    "            combined_df[\"At_Risk_Principle\"] == principle\n",
    "        )\n",
    "        \n",
    "        # Only run chi-square if the table is at least 2x2\n",
    "        if contingency_table.shape[0] > 1 and contingency_table.shape[1] > 1:\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            results.append({\n",
    "                \"Application\": app,\n",
    "                \"At_Risk_Principle\": principle,\n",
    "                \"Chi2\": chi2,\n",
    "                \"p-value\": p\n",
    "            })\n",
    "            print(f\"Chi-Square Test for {app} and {principle}\")\n",
    "            print(f\"Chi2: {chi2:.4f}, p-value: {p:.4f}\")\n",
    "            if p < 0.05:\n",
    "                print(\"Significant association found!\")\n",
    "            else:\n",
    "                print(\"No significant association.\")\n",
    "        else:\n",
    "            print(f\"Insufficient data for Chi-Square test on '{app}' and '{principle}'.\")\n",
    "\n",
    "# Convert results to a DataFrame for visualization\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# Filter for significant results\n",
    "significant_results = result_df[result_df[\"p-value\"] < 0.05]\n",
    "\n",
    "# Create a pivot table and heatmap for significant results\n",
    "pivot_table = significant_results.pivot(index=\"Application\", columns=\"At_Risk_Principle\", values=\"Chi2\")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_table, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar=True)\n",
    "plt.title(\"Significant Associations Between AI Applications and At-Risk Principles (Chi2)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
