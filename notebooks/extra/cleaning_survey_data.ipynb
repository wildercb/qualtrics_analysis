{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Move to the survey results directory \n",
    "data_dir = '../../data'\n",
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed the file with utf-8 encoding.\n",
      "Filtered CSVs created: AI_Study_Prolific_Reviewed_Accepted.csv and rejected_responses.csv\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Remove responses with a yes in the remove column \n",
    "##################\n",
    "\n",
    "import csv\n",
    "\n",
    "def filter_responses(input_file, accepted_file, rejected_file):\n",
    "    encodings = ['utf-8', 'ISO-8859-1', 'windows-1252']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(input_file, 'r', newline='', encoding=encoding) as infile, \\\n",
    "                 open(accepted_file, 'w', newline='', encoding='utf-8') as accepted_outfile, \\\n",
    "                 open(rejected_file, 'w', newline='', encoding='utf-8') as rejected_outfile:\n",
    "                \n",
    "                reader = csv.reader(infile)\n",
    "                accepted_writer = csv.writer(accepted_outfile)\n",
    "                rejected_writer = csv.writer(rejected_outfile)\n",
    "                \n",
    "                # Write header row if it exists\n",
    "                header = next(reader, None)\n",
    "                if header:\n",
    "                    accepted_writer.writerow(header)\n",
    "                    rejected_writer.writerow(header)\n",
    "                \n",
    "                # Filter and write rows\n",
    "                for row in reader:\n",
    "                    if row:\n",
    "                        if not row[0].lower().startswith('yes'):\n",
    "                            accepted_writer.writerow(row)\n",
    "                        else:\n",
    "                            rejected_writer.writerow(row)\n",
    "                \n",
    "                print(f\"Successfully processed the file with {encoding} encoding.\")\n",
    "                return  # Exit the function if successful\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read with {encoding} encoding. Trying next...\")\n",
    "    \n",
    "    print(\"Failed to read the file with any of the specified encodings.\")\n",
    "    raise\n",
    "\n",
    "# Usage\n",
    "input_file = 'AI_Study_Prolific_Reviewed.csv'  # Use the output file from the previous merge script\n",
    "accepted_file = 'AI_Study_Prolific_Reviewed_Accepted.csv'\n",
    "rejected_file = 'rejected_responses.csv'\n",
    "\n",
    "filter_responses(input_file, accepted_file, rejected_file)\n",
    "print(f\"Filtered CSVs created: {accepted_file} and {rejected_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file created: AI_Study_Accepted_With_replacement_codes.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def merge_csv_files(file1_path, file2_path, output_path, skip_rows=0):\n",
    "    # Read the first file\n",
    "    with open(file1_path, 'r', newline='', encoding='utf-8') as file1:\n",
    "        reader1 = csv.reader(file1)\n",
    "        data1 = list(reader1)\n",
    "\n",
    "    # Read the second file, skipping the specified number of rows\n",
    "    with open(file2_path, 'r', newline='', encoding='utf-8') as file2:\n",
    "        reader2 = csv.reader(file2)\n",
    "        # Skip the specified number of rows\n",
    "        for _ in range(skip_rows):\n",
    "            next(reader2, None)\n",
    "        data2 = list(reader2)\n",
    "\n",
    "    # Merge the data\n",
    "    merged_data = data1 + data2\n",
    "\n",
    "    # Write the merged data to the output file\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        writer.writerows(merged_data)\n",
    "\n",
    "    print(f\"Merged file created: {output_path}\")\n",
    "\n",
    "# Usage example\n",
    "file1_path = 'AI_Study_Accepted_With_replacement_codes.csv'\n",
    "file2_path = 'AI_Study_Qualtric_Accepted_With_replacement_codes.csv'\n",
    "output_path = 'AI_Study_Accepted_With_replacement_codes.csv'\n",
    "skip_rows = 1  # Change this to the number of rows you want to skip in the second file\n",
    "\n",
    "merge_csv_files(file1_path, file2_path, output_path, skip_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON created: Updated_Prolific_demographics.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def update_json_with_new_rows(json_file, new_csv_file, updated_json_file):\n",
    "    # Read the existing JSON data\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create a mapping of text entries to their new row numbers from the new CSV\n",
    "    new_row_map = {}\n",
    "    with open(new_csv_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        header = next(reader)  # Skip header row\n",
    "        for row_num, row in enumerate(reader, start=3):  # Start from 3 to match your original indexing\n",
    "            # Assuming the text is in the first column (index 0)\n",
    "            text_value = row[0].strip()  # Strip whitespace for accurate matching\n",
    "            new_row_map[text_value] = row_num  # Map text to its new row number\n",
    "\n",
    "    # Update the JSON data with new row numbers\n",
    "    for entry in data.keys():\n",
    "        for item in data[entry]:\n",
    "            # Strip whitespace from the text for accurate matching\n",
    "            item_text = item['text'].strip()\n",
    "            if item_text in new_row_map:\n",
    "                item['row_number'] = new_row_map[item_text]  # Update the row number\n",
    "\n",
    "    # Write the updated JSON data to a new file\n",
    "    with open(updated_json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Updated JSON created: {updated_json_file}\")\n",
    "\n",
    "# Usage\n",
    "json_file_path = 'Prolific_demographics_other_with_row.json'  # Existing JSON file\n",
    "new_csv_file_path = 'AI_Study_Accepted_With_Replacement_Codes.csv'  # New CSV file with updated entries\n",
    "updated_json_file_path = 'Updated_Prolific_demographics.json'  # Output updated JSON file\n",
    "\n",
    "update_json_with_new_rows(json_file_path, new_csv_file_path, updated_json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question removal completed successfully!\n",
      "Output saved to: Annotator_A_Wilder_Risk_Mitigation.json\n",
      "\n",
      "Statistics:\n",
      "Total questions removed: 1\n",
      "\n",
      "Removed questions:\n",
      "- A.1.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def remove_questions_by_index(input_file, output_file, indices_to_remove):\n",
    "    \"\"\"\n",
    "    Remove specified questions from an annotator book based on their index numbers.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input JSON annotator book\n",
    "        output_file (str): Path where the filtered JSON will be saved\n",
    "        indices_to_remove (list): List of index numbers to remove (e.g., ['4.7', '4.8', '10.1'])\n",
    "                                Can be provided with or without the section letter\n",
    "    \"\"\"\n",
    "    # Read the annotator book\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create new dict for filtered data\n",
    "    filtered_data = {}\n",
    "    removed_questions = []\n",
    "    \n",
    "    # Process indices to handle both formats (e.g., '4.7' and 'B.4.7')\n",
    "    formatted_indices = []\n",
    "    for idx in indices_to_remove:\n",
    "        # Remove section letter if present and any leading/trailing whitespace\n",
    "        cleaned_idx = idx.strip()\n",
    "        if '.' in cleaned_idx and cleaned_idx[0].isalpha():\n",
    "            cleaned_idx = cleaned_idx[cleaned_idx.index('.')+1:]\n",
    "        formatted_indices.append(cleaned_idx)\n",
    "    \n",
    "    # Filter the questions\n",
    "    for question, responses in data.items():\n",
    "        # Extract the index from the question\n",
    "        # Look for the pattern B.X.Y: or X.Y:\n",
    "        question_parts = question.split(':')[0].strip()  # Get everything before the colon\n",
    "        if question_parts[0].isalpha():\n",
    "            question_index = question_parts[question_parts.index('.')+1:]  # Remove section letter\n",
    "        else:\n",
    "            question_index = question_parts\n",
    "            \n",
    "        # If the question index is not in our remove list, keep it\n",
    "        if question_index.strip() not in formatted_indices:\n",
    "            filtered_data[question] = responses\n",
    "        else:\n",
    "            removed_questions.append(question)\n",
    "    \n",
    "    # Write the filtered data to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nQuestion removal completed successfully!\")\n",
    "    print(f\"Output saved to: {output_file}\")\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total questions removed: {len(removed_questions)}\")\n",
    "    print(\"\\nRemoved questions:\")\n",
    "    for q in removed_questions:\n",
    "        print(f\"- {q.split(':')[0].strip()}\")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "indices_to_remove_B = ['B.9.1', 'B.9.2', 'B.9.3','B.9.4','B.10.1', 'B.10.2', 'B.10.3','B.11.1', 'B.11.2', 'B.11.3','B.11.4','B.12.1', 'B.12.2', 'B.12.3','B.12.4','B.13.1', 'B.13.2', 'B.13.3','B.13.4','B.14.1', 'B.14.2', 'B.14.3','B.14.4','B.16.1', 'B.16.2', 'B.16.5','B.16.4','B.17.1', 'B.17.2', 'B.17.3','B.18','C.1','C.2']\n",
    "\n",
    "'''indices_to_remove_A = [\n",
    "    'A.4','A.2.2Â','A.2.1Â _2','A.2.1Â _2',\n",
    "    'A.2.1Â _2','A.2.1Â _1','A.2.1Â _3','A.2.1Â _4','A.2.1Â _5','A.2.1Â _6','A.2.1Â _7','A.2.1Â _8','A.2.1Â _9',\n",
    "    'A.1.5','A.1.4','A.1.2','A.1.3',\n",
    "    'A.4.1','A.4.2','A.4.3'\n",
    "]'''\"AI Study_Qualtric-203-all.csv\"\n",
    "\n",
    "\n",
    "indices_to_remove_B_non_risk_Qs = ['B.1.1','B.2.5_16_TEXT','B.2.6','B.11.1_8_TEXT','B.11.4_8_TEXT','B.11.5','B.11.6','B.12.5','B.12.6','B.13.5','B.13.6','B.14.5','B.14.6','B.9.5','B.9.6','B.10.4','B.16.2_7_TEXT','B.16.3','B.16.4_9_TEXT','B.16.5_5_TEXT','B.16.6','B.17.4','B.17.5','C.3_1_TEXT']\n",
    "\n",
    "indices_to_remove_A_non_risk_Qs = ['A.1.1','A.1.2','A.1.4_14_TEXT','A.1.5_6_TEXT','A.1.6','A.4.3_14_TEXT']\n",
    "\n",
    "indices_to_remove = indices_to_remove_B_non_risk_Qs\n",
    "\n",
    "remove_questions_by_index(\n",
    "    'Annotator_A_all.json',\n",
    "    'Annotator_A_Wilder_Risk_Mitigation.json',\n",
    "    indices_to_remove\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatting completed successfully!\n",
      "Output saved to: ../../data/annotations/json/AI_study_demographics_other.json\n",
      "\n",
      "Formatting completed successfully!\n",
      "Output saved to: ../../data/annotations/json/A_definitions.json\n",
      "\n",
      "Formatting completed successfully!\n",
      "Output saved to: ../../data/annotations/json/A_no_adoption.json\n",
      "\n",
      "Formatting completed successfully!\n",
      "Output saved to: ../../data/annotations/json/A_risk_mitigation.json\n",
      "\n",
      "Formatting completed successfully!\n",
      "Output saved to: ../../data/annotations/json/B_definitions.json\n",
      "\n",
      "Formatting completed successfully!\n",
      "Output saved to: ../../data/annotations/json/B_risk_mitigation.json\n",
      "\n",
      "Formatting completed successfully!\n",
      "Output saved to: ../../data/annotations/json/B_role_am.json\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../../data/annotations/json/demographics_other'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/annotations/json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mformat_annotator_book\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/annotations/json/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/annotations/json/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mformat_annotator_book\u001b[1;34m(input_file, output_file)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mReformats an annotator book JSON file to ensure only themes and codes arrays\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mappear on single lines, without adding extra commas.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    output_file (str): Path where the reformatted JSON will be saved\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# First read the file and parse JSON\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Write initial JSON with standard formatting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '../../data/annotations/json/demographics_other'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def format_annotator_book(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reformats an annotator book JSON file to ensure only themes and codes arrays\n",
    "    appear on single lines, without adding extra commas.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input JSON annotator book\n",
    "        output_file (str): Path where the reformatted JSON will be saved\n",
    "    \"\"\"\n",
    "    # First read the file and parse JSON\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Write initial JSON with standard formatting\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Read the formatted content\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Function to clean and format array content\n",
    "    def format_array(match):\n",
    "        array_content = match.group(1)\n",
    "        # Split into lines and clean up each line\n",
    "        elements = []\n",
    "        for line in array_content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('[') and not line.endswith(']'):\n",
    "                # Remove any trailing commas\n",
    "                if line.endswith(','):\n",
    "                    line = line[:-1]\n",
    "                elements.append(line)\n",
    "        \n",
    "        # Join elements with proper comma separation\n",
    "        return '[' + ', '.join(elements) + ']'\n",
    "    \n",
    "    # Replace theme arrays\n",
    "    content = re.sub(r'\"theme\": \\[(.*?)\\]',\n",
    "                    lambda m: '\"theme\": ' + format_array(m),\n",
    "                    content,\n",
    "                    flags=re.DOTALL)\n",
    "    \n",
    "    # Replace codes arrays\n",
    "    content = re.sub(r'\"codes\": \\[(.*?)\\]',\n",
    "                    lambda m: '\"codes\": ' + format_array(m),\n",
    "                    content,\n",
    "                    flags=re.DOTALL)\n",
    "    \n",
    "    # Write the final formatted content\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "    # Verify the JSON is valid\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            json.load(f)\n",
    "        print(\"\\nFormatting completed successfully!\")\n",
    "        print(\"Output saved to: {}\".format(output_file))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error details:\", str(e))\n",
    "\n",
    "import os \n",
    "for file in os.listdir('../../data/annotations/json'):\n",
    "    # Example usage:\n",
    "    format_annotator_book(\n",
    "        f'../../data/annotations/json/{file}',\n",
    "        f'../../data/annotations/json/{file}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatting completed successfully!\n",
      "Output saved to: Annotator_B_C.json\n",
      "Reordered CSV created: AI_Study_Prolific_Reordered.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def reorder_csv(input_file, output_file):\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        # Read the first 3 rows (including header)\n",
    "        first_three_rows = [next(reader) for _ in range(3)]\n",
    "        \n",
    "        # Separate remaining rows with text in the first column from those without\n",
    "        rows_with_text = []\n",
    "        rows_without_text = []\n",
    "        \n",
    "        for row in reader:\n",
    "            if row and row[0].strip():  # Check if the first column has any non-whitespace text\n",
    "                rows_with_text.append(row)\n",
    "            else:\n",
    "                rows_without_text.append(row)\n",
    "    \n",
    "    # Write the reordered data to the output file\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(first_three_rows)  # Write the first 3 rows as they were\n",
    "        writer.writerows(rows_with_text)\n",
    "        writer.writerows(rows_without_text)\n",
    "\n",
    "    print(f\"Reordered CSV created: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "input_file = 'AI_Study_Prolific_Reviewed.csv'  # Replace with your input CSV file path\n",
    "output_file = 'AI_Study_Prolific_Reordered.csv'  # Output CSV file path\n",
    "\n",
    "reorder_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to themes_and_codes.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def analyze_json(data: Dict) -> Tuple[Dict[str, int], Dict[str, int], int]:\n",
    "    themes_counter = Counter()\n",
    "    codes_counter = Counter()\n",
    "    total_responses = 0\n",
    "\n",
    "    for question, responses in data.items():\n",
    "        for response in responses:\n",
    "            total_responses += 1\n",
    "            # Normalize themes to lowercase before counting\n",
    "            themes_counter.update([theme.lower() for theme in response['theme']])  \n",
    "            codes_counter.update(response['codes'])\n",
    "\n",
    "    return themes_counter, codes_counter, total_responses\n",
    "\n",
    "def organize_results(themes_counter: Counter, codes_counter: Counter, data: Dict) -> Dict:\n",
    "    output = {}\n",
    "    for theme_lower in themes_counter.keys(): \n",
    "        theme = theme_lower.capitalize()  \n",
    "        code_counts = Counter()\n",
    "        for question, responses in data.items():\n",
    "            for response in responses:\n",
    "                if theme_lower in [t.lower() for t in response['theme']]:  # Compare lowercase\n",
    "                    code_counts.update(response['codes'])\n",
    "        output[theme] = [{\"code\": code, \"count\": count} for code, count in code_counts.items()]\n",
    "    return output\n",
    "\n",
    "# Main execution\n",
    "with open('Annotator_B_Ersi-Wilder_Risk_Mitigation.json', 'r', encoding='latin-1') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "themes_counter, codes_counter, total_responses = analyze_json(data)\n",
    "organized_results = organize_results(themes_counter, codes_counter, data)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('themes_and_codes.json', 'w') as outfile:\n",
    "    json.dump(organized_results, outfile, indent=4)\n",
    "\n",
    "print(\"Analysis complete. Results saved to themes_and_codes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created: themes_with_top_example.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load and parse JSON data\n",
    "with open('annotator_B_C.json', 'r',encoding='latin-1') as file:\n",
    "    try:\n",
    "        data = json.load(file)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON. Please check the file format.\")\n",
    "        raise\n",
    "\n",
    "# Dictionary to store themes and examples\n",
    "themes_dict = defaultdict(list)\n",
    "\n",
    "# Process each key in the JSON data\n",
    "for key in data:\n",
    "    entries = data[key]  # Extract list of entries\n",
    "\n",
    "    for entry in entries:\n",
    "        text = entry.get(\"text\", \"\")\n",
    "        themes = entry.get(\"theme\", [])\n",
    "        codes = entry.get(\"codes\", [])\n",
    "        \n",
    "        # Associate text examples with each theme, storing text and code count\n",
    "        for theme in themes:\n",
    "            themes_dict[theme].append({\n",
    "                \"text\": text,\n",
    "                \"codes\": codes\n",
    "            })\n",
    "\n",
    "# Prepare data for CSV output\n",
    "csv_data = []\n",
    "\n",
    "for theme, examples in themes_dict.items():\n",
    "    # Sort examples for each theme by the number of codes in descending order\n",
    "    sorted_examples = sorted(examples, key=lambda x: len(x[\"codes\"]), reverse=True)\n",
    "    \n",
    "    # Get the example with the most codes\n",
    "    if sorted_examples:\n",
    "        top_example = sorted_examples[0]\n",
    "        codes_str = \", \".join(top_example[\"codes\"])  # Combine all codes for column 1\n",
    "        example_text = top_example[\"text\"]  # Column 3: Text of the top entry\n",
    "        theme_desc = \"\"  # Placeholder for description\n",
    "        \n",
    "        # Add to CSV data with all required columns\n",
    "        csv_data.append([codes_str, theme, example_text, theme_desc])\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(csv_data, columns=[\"Codes\", \"Theme\", \"Example Text\", \"Description\"])\n",
    "df.to_csv(\"themes_with_top_example.csv\", index=False)\n",
    "\n",
    "print(\"CSV file created: themes_with_top_example.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    # Remove non-ASCII characters from the text\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "def create_pdf(data_list, pdf_filename):\n",
    "    # Set up the PDF document\n",
    "    doc = SimpleDocTemplate(pdf_filename, pagesize=letter)\n",
    "    elements = []\n",
    "    styles = getSampleStyleSheet()\n",
    "    heading_style = styles['Heading2']\n",
    "    normal_style = styles['BodyText']\n",
    "    \n",
    "    # Increase spacing in heading and normal styles\n",
    "    heading_style.spaceAfter = 14\n",
    "    normal_style.spaceAfter = 8\n",
    "    normal_style.leading = 14  # Increase line height for readability\n",
    "\n",
    "    # Merge the data from both files\n",
    "    merged_data = {}\n",
    "    for data in data_list:\n",
    "        for question, answers in data.items():\n",
    "            if question not in merged_data:\n",
    "                merged_data[question] = answers\n",
    "            else:\n",
    "                merged_data[question].extend(answers)\n",
    "\n",
    "    for question, answers in merged_data.items():\n",
    "        # Remove non-ASCII characters from the question text\n",
    "        clean_question = remove_non_ascii(question)\n",
    "        # Add the question as a heading\n",
    "        elements.append(Paragraph(f\"<b>{clean_question}</b>\", heading_style))\n",
    "        elements.append(Spacer(1, 10))\n",
    "\n",
    "        # Collect answers in table data\n",
    "        table_data = []\n",
    "        for item in answers:\n",
    "            # Remove non-ASCII characters from the answer text\n",
    "            text = remove_non_ascii(item.get('text', '').strip())\n",
    "            if text:\n",
    "                p = Paragraph(text, normal_style)\n",
    "                table_data.append([p])\n",
    "\n",
    "        # Create table if there are answers\n",
    "        if table_data:\n",
    "            # Create the table with improved padding and alternate row colors\n",
    "            table = Table(table_data, colWidths=[6.5*inch])\n",
    "            row_count = len(table_data)\n",
    "            bg_colors = [colors.whitesmoke, colors.lightgrey]\n",
    "            ts = TableStyle(\n",
    "                [('BACKGROUND', (0, i), (-1, i), bg_colors[i % 2]) for i in range(row_count)] +\n",
    "                [('VALIGN', (0, 0), (-1, -1), 'TOP'),\n",
    "                 ('FONT', (0, 0), (-1, -1), 'Helvetica', 10),\n",
    "                 ('LEFTPADDING', (0, 0), (-1, -1), 8),\n",
    "                 ('RIGHTPADDING', (0, 0), (-1, -1), 8),\n",
    "                 ('TOPPADDING', (0, 0), (-1, -1), 6),\n",
    "                 ('BOTTOMPADDING', (0, 0), (-1, -1), 6)]\n",
    "            )\n",
    "            table.setStyle(ts)\n",
    "            elements.append(table)\n",
    "            elements.append(Spacer(1, 20))\n",
    "\n",
    "    # Build the PDF\n",
    "    doc.build(elements)\n",
    "\n",
    "# Read JSON data from two files\n",
    "file_names = ['All_As.json']\n",
    "data_list = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        data_list.append(data)\n",
    "\n",
    "create_pdf(data_list, 'Annotations/all_survey_results.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split completed successfully!\n",
      "Rows with text in columns 42-60: 107 written to track_a.csv\n",
      "Remaining rows: 307 written to track_b.csv\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Split CSV with A track and B track\n",
    "########################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def split_csv(input_file, track_a_file, track_b_file):\n",
    "    \"\"\"\n",
    "    Splits the input CSV into two separate CSV files (track_a and track_b) based on the presence\n",
    "    of text in columns 42 to 60. Both output files retain the first two rows of the input file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file (str): Path to the input CSV file.\n",
    "    - track_a_file (str): Path for the first output CSV file (track_a).\n",
    "    - track_b_file (str): Path for the second output CSV file (track_b).\n",
    "    \"\"\"\n",
    "    # Read the CSV file without assuming a header and treat all data as strings\n",
    "    df = pd.read_csv(input_file, dtype=str, keep_default_na=False, header=None)\n",
    "\n",
    "    # Extract the first two rows to serve as headers for both output files\n",
    "    header = df.iloc[:2]\n",
    "\n",
    "    # Extract the remaining data\n",
    "    data = df.iloc[2:]\n",
    "\n",
    "    # Define the column indices for columns 42 to 60 (0-based indexing)\n",
    "    col_start = 55  # Column 42\n",
    "    col_end = 65   # Column 60 (exclusive in pandas iloc)\n",
    "\n",
    "    # Determine the actual end index based on the dataframe's columns\n",
    "    max_cols = data.shape[1]\n",
    "    col_end = min(col_end, max_cols)\n",
    "\n",
    "    if col_start >= max_cols:\n",
    "        # If there are fewer than 42 columns, no rows will match the condition\n",
    "        mask = pd.Series([False] * len(data))\n",
    "    else:\n",
    "        # Create a boolean mask where True indicates that at least one of the columns 42-60 has text\n",
    "        mask = data.iloc[:, col_start:col_end].apply(lambda row: row.str.strip().astype(bool), axis=1).any(axis=1)\n",
    "\n",
    "    # Split the data based on the mask\n",
    "    track_a_data = data[mask]\n",
    "    track_b_data = data[~mask]\n",
    "\n",
    "    # Concatenate the headers with the respective data\n",
    "    track_a = pd.concat([header, track_a_data], ignore_index=True)\n",
    "    track_b = pd.concat([header, track_b_data], ignore_index=True)\n",
    "\n",
    "    # Write the split data to the respective CSV files without headers and index\n",
    "    track_a.to_csv(track_a_file, index=False, header=False)\n",
    "    track_b.to_csv(track_b_file, index=False, header=False)\n",
    "\n",
    "    print(f\"Split completed successfully!\")\n",
    "    print(f\"Rows with text in columns 42-60: {len(track_a_data)} written to {track_a_file}\")\n",
    "    print(f\"Remaining rows: {len(track_b_data)} written to {track_b_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your file paths\n",
    "    input_csv = 'AI_Study_Accepted_With_Replacement_Codes.csv'       # Replace with your input file path\n",
    "    output_track_a = 'track_a.csv'  # Desired output path for track A\n",
    "    output_track_b = 'track_b.csv'  # Desired output path for track B\n",
    "\n",
    "    # Call the function to perform the split\n",
    "    split_csv(input_csv, output_track_a, output_track_b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
